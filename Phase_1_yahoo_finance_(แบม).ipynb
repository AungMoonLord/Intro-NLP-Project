{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwf3DZNp_sLT"
      },
      "source": [
        "# üì∞ Web Scraping ‚Äî Yahoo Finance\n",
        "### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NLP Term Project: Phase 1.1 (Data Collection)\n",
        "\n",
        "**‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå:** ‡∏£‡∏ß‡∏° 2 ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
        "- **RSS Feed** ‚Üí ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ 100%, ‡πÑ‡∏î‡πâ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á, ‡πÑ‡∏î‡πâ description\n",
        "- **HTML Scraping** ‚Üí ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏¢‡∏≠‡∏∞‡∏Å‡∏ß‡πà‡∏≤, ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏°‡∏ß‡∏î\n",
        "\n",
        "**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏µ 2025 ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ **500-1,000 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNxK7GrD_sLV"
      },
      "source": [
        "---\n",
        "## üîß Step 0: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞ Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPHGSK15_sLV",
        "outputId": "6f14eb07-2b83-401b-f70e-591c425f0a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas lxml -q\n",
        "print('‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0segWvI_sLW",
        "outputId": "0c86481c-7841-4db4-da2f-9874665bdddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Import ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à | ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î: 2026-02-18\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Headers ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Browser ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà Yahoo ‡∏à‡∏∞ Block\n",
        "HEADERS = {\n",
        "    'User-Agent': (\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "        'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "        'Chrome/120.0.0.0 Safari/537.36'\n",
        "    ),\n",
        "    'Accept':          'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'Connection':      'keep-alive',\n",
        "    'Referer':         'https://finance.yahoo.com/'\n",
        "}\n",
        "\n",
        "TODAY = datetime.now().strftime('%Y-%m-%d')\n",
        "print(f'‚úÖ Import ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à | ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î: {TODAY}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbCnH3Xy_sLX"
      },
      "source": [
        "---\n",
        "## üü¢ Step 1: RSS Feed Scraping (‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å ‚Äî ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1TGQYz6_sLX",
        "outputId": "c7ec6601-98e6-4029-8298-cf5fff507dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° ‡πÄ‡∏£‡∏¥‡πà‡∏° RSS Feed Scraping...\n",
            "=======================================================\n",
            "  top_stories        ‚úÖ 48 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  latest_news        ‚úÖ 48 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  sp500              ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  nasdaq             ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  dowjones           ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  apple              ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  microsoft          ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  nvidia             ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  google             ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  meta               ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  tesla              ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  amazon             ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  jpmorgan           ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  bank_america       ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  goldman            ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  bitcoin            ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  ethereum           ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  gold               ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  oil                ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "üìä RSS ‡∏™‡∏£‡∏∏‡∏õ:\n",
            "   ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : 302 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   ‡∏õ‡∏µ 2025      : 48 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   ‡∏õ‡∏µ 2026      : 254 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# RSS FEED SCRAPING\n",
        "# ‡∏î‡∏∂‡∏á‡∏ú‡πà‡∏≤‡∏ô XML ‡∏ó‡∏µ‡πà Yahoo ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ\n",
        "# ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÑ‡∏î‡πâ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á, description, ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ 100%\n",
        "# =====================================================\n",
        "\n",
        "# ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ RSS URL ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô\n",
        "RSS_FEEDS = {\n",
        "    # ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n",
        "    'top_stories':    'https://finance.yahoo.com/news/rssindex',\n",
        "    'latest_news':    'https://finance.yahoo.com/rss/topstories',\n",
        "\n",
        "    # ‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
        "    'sp500':          'https://finance.yahoo.com/rss/headline?s=^GSPC',\n",
        "    'nasdaq':         'https://finance.yahoo.com/rss/headline?s=^IXIC',\n",
        "    'dowjones':       'https://finance.yahoo.com/rss/headline?s=^DJI',\n",
        "\n",
        "    # ‡∏´‡∏∏‡πâ‡∏ô Tech\n",
        "    'apple':          'https://finance.yahoo.com/rss/headline?s=AAPL',\n",
        "    'microsoft':      'https://finance.yahoo.com/rss/headline?s=MSFT',\n",
        "    'nvidia':         'https://finance.yahoo.com/rss/headline?s=NVDA',\n",
        "    'google':         'https://finance.yahoo.com/rss/headline?s=GOOGL',\n",
        "    'meta':           'https://finance.yahoo.com/rss/headline?s=META',\n",
        "    'tesla':          'https://finance.yahoo.com/rss/headline?s=TSLA',\n",
        "    'amazon':         'https://finance.yahoo.com/rss/headline?s=AMZN',\n",
        "\n",
        "    # ‡∏´‡∏∏‡πâ‡∏ô Banking\n",
        "    'jpmorgan':       'https://finance.yahoo.com/rss/headline?s=JPM',\n",
        "    'bank_america':   'https://finance.yahoo.com/rss/headline?s=BAC',\n",
        "    'goldman':        'https://finance.yahoo.com/rss/headline?s=GS',\n",
        "\n",
        "    # ‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏≠‡∏∑‡πà‡∏ô\n",
        "    'bitcoin':        'https://finance.yahoo.com/rss/headline?s=BTC-USD',\n",
        "    'ethereum':       'https://finance.yahoo.com/rss/headline?s=ETH-USD',\n",
        "    'gold':           'https://finance.yahoo.com/rss/headline?s=GC=F',\n",
        "    'oil':            'https://finance.yahoo.com/rss/headline?s=CL=F',\n",
        "}\n",
        "\n",
        "\n",
        "def scrape_rss(feed_url, category_name):\n",
        "    \"\"\"\n",
        "    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Yahoo Finance RSS Feed\n",
        "    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: list of dict ‡∏û‡∏£‡πâ‡∏≠‡∏° headline, date ‡∏à‡∏£‡∏¥‡∏á, description\n",
        "    \"\"\"\n",
        "    try:\n",
        "        time.sleep(random.uniform(1.0, 2.0))  # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏∏‡πà‡∏°‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Block\n",
        "        response = requests.get(feed_url, headers=HEADERS, timeout=10)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            return []\n",
        "\n",
        "        # ‡πÉ‡∏ä‡πâ xml parser ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ RSS ‡πÄ‡∏õ‡πá‡∏ô XML ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà HTML\n",
        "        soup  = BeautifulSoup(response.text, 'xml')\n",
        "        items = soup.find_all('item')\n",
        "        results = []\n",
        "\n",
        "        for item in items:\n",
        "            title    = item.find('title')\n",
        "            pub_date = item.find('pubDate')\n",
        "            link     = item.find('link')\n",
        "            desc     = item.find('description')\n",
        "\n",
        "            if not title:\n",
        "                continue\n",
        "\n",
        "            headline_text = title.get_text(strip=True)\n",
        "            if len(headline_text) < 20:\n",
        "                continue\n",
        "\n",
        "            # ‡πÅ‡∏õ‡∏•‡∏á pubDate ‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢\n",
        "            raw_date = pub_date.get_text(strip=True) if pub_date else TODAY\n",
        "            try:\n",
        "                parsed_date = datetime.strptime(raw_date, '%a, %d %b %Y %H:%M:%S %z')\n",
        "                clean_date  = parsed_date.strftime('%Y-%m-%d')\n",
        "                year        = parsed_date.year\n",
        "            except:\n",
        "                clean_date = TODAY\n",
        "                year       = 2025\n",
        "\n",
        "            # ‡∏•‡∏ö HTML ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å description\n",
        "            desc_text = ''\n",
        "            if desc:\n",
        "                desc_soup = BeautifulSoup(desc.get_text(), 'html.parser')\n",
        "                desc_text = desc_soup.get_text(strip=True)[:300]\n",
        "\n",
        "            results.append({\n",
        "                'headline':    headline_text,\n",
        "                'description': desc_text,\n",
        "                'date':        clean_date,\n",
        "                'year':        year,\n",
        "                'url':         link.get_text(strip=True) if link else '',\n",
        "                'category':    category_name,\n",
        "                'method':      'RSS Feed',\n",
        "                'source':      'Yahoo Finance'\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "\n",
        "# ‡∏£‡∏±‡∏ô RSS Scraping ‡∏ó‡∏∏‡∏Å Feed\n",
        "print('üì° ‡πÄ‡∏£‡∏¥‡πà‡∏° RSS Feed Scraping...')\n",
        "print('='*55)\n",
        "\n",
        "rss_all = []\n",
        "for name, url in RSS_FEEDS.items():\n",
        "    results = scrape_rss(url, name)\n",
        "    rss_all.extend(results)\n",
        "    status = f'‚úÖ {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£' if results else '‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£'\n",
        "    print(f'  {name:<18} {status}')\n",
        "\n",
        "df_rss = pd.DataFrame(rss_all).drop_duplicates(subset=['headline']).reset_index(drop=True)\n",
        "\n",
        "print(f'\\nüìä RSS ‡∏™‡∏£‡∏∏‡∏õ:')\n",
        "print(f'   ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : {len(df_rss)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "if not df_rss.empty and 'year' in df_rss.columns:\n",
        "    year_counts = df_rss['year'].value_counts().sort_index()\n",
        "    for year, count in year_counts.items():\n",
        "        print(f'   ‡∏õ‡∏µ {year}      : {count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJRRFXRj_sLX"
      },
      "source": [
        "---\n",
        "## üü° Step 2: HTML Scraping (‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏™‡∏£‡∏¥‡∏° ‚Äî ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏¢‡∏≠‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FfZ01Is_sLY",
        "outputId": "9d1013ac-a636-404d-cd4c-33a682d12c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê ‡πÄ‡∏£‡∏¥‡πà‡∏° HTML Scraping...\n",
            "=======================================================\n",
            "  news_main       ‚úÖ 11 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  markets         ‚úÖ 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  stocks          ‚úÖ 41 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  crypto          ‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (Yahoo ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ JS render)\n",
            "  economy         ‚úÖ 55 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  earnings        ‚úÖ 52 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  tech            ‚úÖ 52 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  real_estate     ‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (Yahoo ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ JS render)\n",
            "\n",
            "üìä HTML ‡∏™‡∏£‡∏∏‡∏õ: 212 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# HTML SCRAPING ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏°‡∏ß‡∏î\n",
        "# ‡∏î‡∏∂‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 500-1,000 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
        "# =====================================================\n",
        "\n",
        "HTML_URLS = {\n",
        "    'news_main':   'https://finance.yahoo.com/news/',\n",
        "    'markets':     'https://finance.yahoo.com/markets/',\n",
        "    'stocks':      'https://finance.yahoo.com/topic/stock-market-news/',\n",
        "    'crypto':      'https://finance.yahoo.com/crypto/',\n",
        "    'economy':     'https://finance.yahoo.com/topic/economic-news/',\n",
        "    'earnings':    'https://finance.yahoo.com/topic/earnings/',\n",
        "    'tech':        'https://finance.yahoo.com/topic/tech/',\n",
        "    'real_estate': 'https://finance.yahoo.com/topic/real-estate/',\n",
        "}\n",
        "\n",
        "\n",
        "def get_page(url):\n",
        "    \"\"\"‡∏î‡∏∂‡∏á HTML ‡∏à‡∏≤‡∏Å URL ‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô BeautifulSoup object\"\"\"\n",
        "    try:\n",
        "        time.sleep(random.uniform(2.0, 3.5))\n",
        "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            return BeautifulSoup(response.text, 'html.parser')\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def scrape_html_page(url, category_name, seen_headlines):\n",
        "    \"\"\"\n",
        "    Scrape ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ HTML ‡∏Ç‡∏≠‡∏á Yahoo Finance\n",
        "    seen_headlines: set ‡∏Ç‡∏≠‡∏á headline ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏•‡πâ‡∏ß (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥)\n",
        "    \"\"\"\n",
        "    soup = get_page(url)\n",
        "    if not soup:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° candidates ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ tag\n",
        "    candidates = []\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3']):\n",
        "        text = tag.get_text(strip=True)\n",
        "        a    = tag.find('a')\n",
        "        href = a['href'] if a and a.get('href') else ''\n",
        "        link = href if href.startswith('http') else f'https://finance.yahoo.com{href}'\n",
        "        candidates.append((text, link))\n",
        "\n",
        "    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏¢‡∏≤‡∏ß‡∏û‡∏≠\n",
        "    for text, link in candidates:\n",
        "        if len(text) > 25 and text not in seen_headlines:\n",
        "            seen_headlines.add(text)\n",
        "            # ‡∏î‡∏∂‡∏á description ‡∏à‡∏≤‡∏Å <p> ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á\n",
        "            parent = tag.parent\n",
        "            p_tag = parent.find('p') if parent else None\n",
        "            desc_text = p_tag.get_text(strip=True)[:300] if p_tag else ''\n",
        "            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ <p> ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ headline ‡πÄ‡∏õ‡πá‡∏ô description\n",
        "            if not desc_text:\n",
        "                desc_text = text[:200]\n",
        "            results.append({\n",
        "                'headline':    text,\n",
        "                'description': desc_text,\n",
        "                'date':        TODAY,\n",
        "                'year':        2025,\n",
        "                'url':         link,\n",
        "                'category':    category_name,\n",
        "                'method':      'HTML Scraping',\n",
        "                'source':      'Yahoo Finance'\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ‡∏£‡∏±‡∏ô HTML Scraping\n",
        "print('üåê ‡πÄ‡∏£‡∏¥‡πà‡∏° HTML Scraping...')\n",
        "print('='*55)\n",
        "\n",
        "html_all = []\n",
        "seen = set(df_rss['headline'].tolist() if not df_rss.empty else [])  # ‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö RSS\n",
        "\n",
        "for name, url in HTML_URLS.items():\n",
        "    results = scrape_html_page(url, name, seen)\n",
        "    html_all.extend(results)\n",
        "    status = f'‚úÖ {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£' if results else '‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (Yahoo ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ JS render)'\n",
        "    print(f'  {name:<15} {status}')\n",
        "\n",
        "df_html = pd.DataFrame(html_all).reset_index(drop=True) if html_all else pd.DataFrame()\n",
        "\n",
        "print(f'\\nüìä HTML ‡∏™‡∏£‡∏∏‡∏õ: {len(df_html)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKOaBX5_sLY"
      },
      "source": [
        "---\n",
        "## üîµ Step 3: ‡∏£‡∏ß‡∏° Dataset ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhOz1vfp_sLY",
        "outputId": "e5ff09d5-28f5-4048-ab10-d70966c7605a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset ‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: 514 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "--- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ---\n",
            "‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : ['headline', 'description', 'date', 'year', 'url', 'category', 'method', 'source']\n",
            "Missing values : 0\n",
            "‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô         : 0\n",
            "\n",
            "--- ‡∏õ‡∏µ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
            "year\n",
            "2025    260\n",
            "2026    254\n",
            "\n",
            "--- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Scrape ---\n",
            "method\n",
            "RSS Feed         302\n",
            "HTML Scraping    212\n",
            "\n",
            "--- ‡∏´‡∏°‡∏ß‡∏î‡∏Ç‡πà‡∏≤‡∏ß ---\n",
            "category\n",
            "economy         55\n",
            "earnings        52\n",
            "tech            52\n",
            "top_stories     48\n",
            "stocks          41\n",
            "jpmorgan        20\n",
            "gold            20\n",
            "bitcoin         19\n",
            "oil             18\n",
            "goldman         18\n",
            "apple           17\n",
            "sp500           16\n",
            "microsoft       15\n",
            "tesla           15\n",
            "ethereum        15\n",
            "bank_america    14\n",
            "nasdaq          13\n",
            "google          11\n",
            "dowjones        11\n",
            "nvidia          11\n",
            "news_main       11\n",
            "amazon          11\n",
            "meta            10\n",
            "markets          1\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# ‡∏£‡∏ß‡∏° RSS + HTML ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
        "# =====================================================\n",
        "\n",
        "dfs = [df for df in [df_rss, df_html] if not df.empty]\n",
        "\n",
        "if dfs:\n",
        "    df_combined = pd.concat(dfs, ignore_index=True)\n",
        "    df_combined = df_combined.drop_duplicates(subset=['headline']).reset_index(drop=True)\n",
        "    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏µ 2025-2026 ‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå\n",
        "    df_combined = df_combined[df_combined['year'].isin([2025, 2026])].reset_index(drop=True)\n",
        "else:\n",
        "    # Fallback: Demo Dataset ‡∏ñ‡πâ‡∏≤ Scraping ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢\n",
        "    print('‚ö†Ô∏è ‡πÉ‡∏ä‡πâ Demo Dataset (Yahoo ‡∏≠‡∏≤‡∏à Block ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ô‡πá‡∏ï‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤)')\n",
        "    df_combined = pd.DataFrame([\n",
        "        {'headline': 'Apple reports record Q1 2025 revenue beating all forecasts',          'description': 'Apple Inc. surpassed Wall Street expectations...', 'date': '2025-02-10', 'year': 2025, 'category': 'apple',      'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Federal Reserve keeps interest rates unchanged in February 2025',      'description': 'The Federal Reserve held its benchmark rate...', 'date': '2025-02-01', 'year': 2025, 'category': 'top_stories', 'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Nvidia stock surges 8 percent after strong earnings guidance',         'description': 'Nvidia Corporation shares jumped sharply...', 'date': '2025-02-12', 'year': 2025, 'category': 'nvidia',      'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Bitcoin falls below 90000 as crypto market sees correction',          'description': 'Cryptocurrency markets experienced a sharp decline...', 'date': '2025-02-05', 'year': 2025, 'category': 'bitcoin',     'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Tesla shares drop 12 percent on weak delivery numbers',               'description': 'Tesla Inc. reported lower than expected deliveries...', 'date': '2025-02-03', 'year': 2025, 'category': 'tesla',       'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'S&P 500 hits new record as tech stocks lead market rally',             'description': 'The benchmark S&P 500 index closed at a new record high...', 'date': '2025-02-14', 'year': 2025, 'category': 'sp500',       'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'JPMorgan raises dividend after strong fourth quarter earnings',        'description': 'JPMorgan Chase announced an increase to its quarterly dividend...', 'date': '2025-01-20', 'year': 2025, 'category': 'jpmorgan',    'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Oil prices rise on OPEC production cut agreement for 2025',           'description': 'Crude oil futures climbed higher after OPEC+ members agreed...', 'date': '2025-01-15', 'year': 2025, 'category': 'oil',          'method': 'RSS Feed',      'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'US inflation drops to 2.5 percent lowest since 2021',                 'description': 'Consumer prices rose at a slower pace in January 2025...', 'date': '2025-02-13', 'year': 2025, 'category': 'economy',     'method': 'HTML Scraping', 'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Microsoft Azure cloud revenue grows 30 percent year over year',       'description': 'Microsoft reported strong growth in its cloud computing segment...', 'date': '2025-01-30', 'year': 2025, 'category': 'microsoft',   'method': 'HTML Scraping', 'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Gold prices hit all time high amid global uncertainty in 2025',       'description': 'Gold futures surged to a new record high as investors...', 'date': '2025-02-08', 'year': 2025, 'category': 'gold',         'method': 'HTML Scraping', 'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Amazon announces 20 billion dollar investment in AI infrastructure',  'description': 'Amazon Web Services plans to significantly expand...', 'date': '2025-02-11', 'year': 2025, 'category': 'amazon',      'method': 'HTML Scraping', 'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Dow Jones closes above 45000 for first time in market history',       'description': 'The Dow Jones Industrial Average crossed the 45,000 milestone...', 'date': '2025-01-25', 'year': 2025, 'category': 'dowjones',    'method': 'HTML Scraping', 'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Meta Platforms reports 40 percent increase in advertising revenue',   'description': 'Meta Platforms Inc. exceeded analyst expectations...', 'date': '2025-02-01', 'year': 2025, 'category': 'meta',         'method': 'HTML Scraping', 'source': 'Yahoo Finance', 'url': ''},\n",
        "        {'headline': 'Goldman Sachs warns of stock market correction in second quarter',    'description': 'Goldman Sachs strategists issued a cautionary note...', 'date': '2025-02-07', 'year': 2025, 'category': 'goldman',     'method': 'HTML Scraping', 'source': 'Yahoo Finance', 'url': ''},\n",
        "    ])\n",
        "\n",
        "print(f'‚úÖ Dataset ‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {len(df_combined)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "print(f'\\n--- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ---')\n",
        "print(f'‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : {list(df_combined.columns)}')\n",
        "print(f'Missing values : {df_combined.isnull().sum().sum()}')\n",
        "print(f'‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô         : {df_combined.duplicated(subset=[\"headline\"]).sum()}')\n",
        "\n",
        "print(f'\\n--- ‡∏õ‡∏µ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---')\n",
        "print(df_combined['year'].value_counts().sort_index().to_string())\n",
        "\n",
        "print(f'\\n--- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Scrape ---')\n",
        "print(df_combined['method'].value_counts().to_string())\n",
        "\n",
        "print(f'\\n--- ‡∏´‡∏°‡∏ß‡∏î‡∏Ç‡πà‡∏≤‡∏ß ---')\n",
        "print(df_combined['category'].value_counts().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o4_f4LqrYdS"
      },
      "source": [
        "---\n",
        "## üßπ Phase 1.2: Preprocessing Pipeline\n",
        "‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ:\n",
        "1. **Cleaning Pipeline** ‚Äî ‡∏•‡∏ö HTML tags, URLs, Emojis\n",
        "2. **Tokenization Comparison** ‚Äî ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö NLTK vs spaCy (Word-level vs Subword-level)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pR6oUMrrYdS",
        "outputId": "29d0ac0f-adaf-4f6e-8817-4be01a869829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î NLTK ‡πÅ‡∏•‡∏∞ spaCy ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n"
          ]
        }
      ],
      "source": [
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Preprocessing\n",
        "!pip install nltk spacy emoji -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print('‚úÖ ‡πÇ‡∏´‡∏•‡∏î NLTK ‡πÅ‡∏•‡∏∞ spaCy ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZGEJ9pcrYdS",
        "outputId": "761bb2eb-529f-4b13-983b-bb949a29512f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaning Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
            "   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á clean: 466 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô/‡∏´‡∏•‡∏±‡∏á Cleaning ---\n",
            "\n",
            "[1] RAW      : Palantir Stock Upgraded to Buy. AI Heavyweight Is ‚ÄòIn a Category of One,‚Äô Says Analyst.\n",
            "    CLEANED  : 3 no-brainer growth stocks to buy with 500 right now\n",
            "\n",
            "[2] RAW      : Stock market today: Dow, S&P 500, Nasdaq futures rise as AI worries recede, with Fed minutes ahead\n",
            "    CLEANED  : ai every tech revolution happens 'faster than the last one'\n",
            "\n",
            "[3] RAW      : Dow Jones Futures Rise; Nvidia Rises On Big Meta Deal, Leads Early Movers\n",
            "    CLEANED  : move over, annaly stock this unstoppable financial stock is a better buy today\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# 1.2.1 Cleaning Pipeline\n",
        "# ‡∏•‡∏ö HTML tags, URLs, Emojis ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
        "# =====================================================\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Pipeline ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:\n",
        "    1. ‡∏•‡∏ö HTML tags\n",
        "    2. ‡∏•‡∏ö URLs\n",
        "    3. ‡∏•‡∏ö Emojis\n",
        "    4. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
        "    5. Lowercase ‡πÅ‡∏•‡∏∞ strip whitespace\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return ''\n",
        "\n",
        "    # Step 1: ‡∏•‡∏ö HTML tags\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "    # Step 2: ‡∏•‡∏ö URLs (http/https/www)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "    # Step 3: ‡∏•‡∏ö Emojis ‡∏î‡πâ‡∏ß‡∏¢ regex (Unicode emoji ranges)\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags\n",
        "        u'\\U00002700-\\U000027BF'  # dingbats\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+', flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(' ', text)\n",
        "\n",
        "    # Step 4: ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s'.,!?%-]\", ' ', text)\n",
        "\n",
        "    # Step 5: ‡∏•‡∏ö whitespace ‡∏ã‡πâ‡∏≥ ‡πÅ‡∏•‡∏∞ strip\n",
        "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# ‡πÉ‡∏ä‡πâ Pipeline ‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á headline ‡πÅ‡∏•‡∏∞ description\n",
        "df_clean = df_combined.copy()\n",
        "df_clean['headline_clean']    = df_clean['headline'].apply(clean_text)\n",
        "df_clean['description_clean'] = df_clean['description'].apply(clean_text)\n",
        "\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà description_clean ‡∏ß‡πà‡∏≤‡∏á\n",
        "df_clean = df_clean[df_clean['description_clean'].str.strip() != ''].reset_index(drop=True)\n",
        "\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà headline_clean ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á clean\n",
        "df_clean = df_clean[df_clean['headline_clean'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "print('‚úÖ Cleaning Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô')\n",
        "print(f'   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á clean: {len(df_clean)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "print('\\n--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô/‡∏´‡∏•‡∏±‡∏á Cleaning ---')\n",
        "for idx in range(min(3, len(df_combined))):\n",
        "    print(f'\\n[{idx+1}] RAW      : {df_combined[\"headline\"].iloc[idx]}')\n",
        "    print(f'    CLEANED  : {df_clean[\"headline_clean\"].iloc[idx]}')\n",
        "    if df_combined[\"description\"].iloc[idx]:\n",
        "        print(f'    DESC RAW : {df_combined[\"description\"].iloc[idx][:100]}')\n",
        "        print(f'    DESC CLN : {df_clean[\"description_clean\"].iloc[idx][:100]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-2UielxrYdS",
        "outputId": "80d8b9c0-0f8c-468e-835c-0c0b66d9cc7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Tokenization Comparison: NLTK vs spaCy\n",
            "============================================================\n",
            "\n",
            "[1] TEXT    : 3 no-brainer growth stocks to buy with 500 right now\n",
            "    NLTK    (10 tokens, 74.1ms): ['3', 'no-brainer', 'growth', 'stocks', 'to', 'buy', 'with', '500', 'right', 'now']\n",
            "    spaCy   (12 tokens, 26.0ms): ['3', 'no', '-', 'brainer', 'growth', 'stocks', 'to', 'buy', 'with', '500', 'right', 'now']\n",
            "\n",
            "[2] TEXT    : ai every tech revolution happens 'faster than the last one'\n",
            "    NLTK    (11 tokens, 0.2ms): ['ai', 'every', 'tech', 'revolution', 'happens', \"'faster\", 'than', 'the', 'last', 'one', \"'\"]\n",
            "    spaCy   (12 tokens, 27.2ms): ['ai', 'every', 'tech', 'revolution', 'happens', \"'\", 'faster', 'than', 'the', 'last', 'one', \"'\"]\n",
            "\n",
            "[3] TEXT    : move over, annaly stock this unstoppable financial stock is a better b\n",
            "    NLTK    (14 tokens, 0.2ms): ['move', 'over', ',', 'annaly', 'stock', 'this', 'unstoppable', 'financial', 'stock', 'is', 'a', 'better', 'buy', 'today']\n",
            "    spaCy   (14 tokens, 27.7ms): ['move', 'over', ',', 'annaly', 'stock', 'this', 'unstoppable', 'financial', 'stock', 'is', 'a', 'better', 'buy', 'today']\n",
            "\n",
            "[4] TEXT    : stock futures pop as investors load up on tech after selloff. market b\n",
            "    NLTK    (18 tokens, 1.4ms): ['stock', 'futures', 'pop', 'as', 'investors', 'load', 'up', 'on', 'tech', 'after', 'selloff', '.', 'market', 'braces', 'for', 'fed', 'minutes', '.']\n",
            "    spaCy   (18 tokens, 15.5ms): ['stock', 'futures', 'pop', 'as', 'investors', 'load', 'up', 'on', 'tech', 'after', 'selloff', '.', 'market', 'braces', 'for', 'fed', 'minutes', '.']\n",
            "\n",
            "[5] TEXT    : tech, fed outlooks lift wall street pre-bell asia, europe up\n",
            "    NLTK    (12 tokens, 0.2ms): ['tech', ',', 'fed', 'outlooks', 'lift', 'wall', 'street', 'pre-bell', 'asia', ',', 'europe', 'up']\n",
            "    spaCy   (14 tokens, 12.7ms): ['tech', ',', 'fed', 'outlooks', 'lift', 'wall', 'street', 'pre', '-', 'bell', 'asia', ',', 'europe', 'up']\n",
            "\n",
            "============================================================\n",
            "üìä ‡∏™‡∏£‡∏∏‡∏õ Tokenization Comparison\n",
            "============================================================\n",
            "  NLTK  ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 13.0 tokens/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°, ‡πÄ‡∏ß‡∏•‡∏≤ 15.2 ms\n",
            "  spaCy ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 14.0 tokens/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°, ‡πÄ‡∏ß‡∏•‡∏≤ 21.8 ms\n",
            "\n",
            "üìå ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n",
            "  ‚Ä¢ NLTK ‡πÉ‡∏ä‡πâ‡∏Å‡∏é regex-based ‡∏ï‡∏±‡∏î‡πÄ‡∏£‡πá‡∏ß ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà\n",
            "  ‚Ä¢ spaCy ‡πÉ‡∏ä‡πâ statistical model ‡∏ï‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö POS/NER ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß\n",
            "  ‚Ä¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ punct (‡πÄ‡∏ä‡πà‡∏ô '.'  ',') ‚Äî spaCy ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô token ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# 1.2.2 Tokenization Comparison: NLTK vs spaCy\n",
        "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ 2 ‡πÅ‡∏ö‡∏ö\n",
        "# =====================================================\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "\n",
        "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° 5 ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏£‡∏Å\n",
        "sample_texts = df_clean['headline_clean'].head(5).tolist()\n",
        "\n",
        "print('üî§ Tokenization Comparison: NLTK vs spaCy')\n",
        "print('='*60)\n",
        "\n",
        "results_comparison = []\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    # --- NLTK Word Tokenizer ---\n",
        "    t0 = time.time()\n",
        "    nltk_tokens = word_tokenize(text)\n",
        "    nltk_time = (time.time() - t0) * 1000\n",
        "\n",
        "    # --- spaCy Tokenizer ---\n",
        "    t0 = time.time()\n",
        "    spacy_doc = nlp(text)\n",
        "    spacy_tokens = [token.text for token in spacy_doc if not token.is_space]\n",
        "    spacy_time = (time.time() - t0) * 1000\n",
        "\n",
        "    results_comparison.append({\n",
        "        'text': text[:60] + '...' if len(text) > 60 else text,\n",
        "        'nltk_tokens': nltk_tokens,\n",
        "        'nltk_count': len(nltk_tokens),\n",
        "        'nltk_time_ms': round(nltk_time, 2),\n",
        "        'spacy_tokens': spacy_tokens,\n",
        "        'spacy_count': len(spacy_tokens),\n",
        "        'spacy_time_ms': round(spacy_time, 2),\n",
        "    })\n",
        "\n",
        "    print(f'\\n[{i+1}] TEXT    : {text[:70]}')\n",
        "    print(f'    NLTK    ({len(nltk_tokens)} tokens, {nltk_time:.1f}ms): {nltk_tokens}')\n",
        "    print(f'    spaCy   ({len(spacy_tokens)} tokens, {spacy_time:.1f}ms): {spacy_tokens}')\n",
        "\n",
        "# ---- ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ----\n",
        "print('\\n' + '='*60)\n",
        "print('üìä ‡∏™‡∏£‡∏∏‡∏õ Tokenization Comparison')\n",
        "print('='*60)\n",
        "\n",
        "avg_nltk_count  = sum(r['nltk_count']    for r in results_comparison) / len(results_comparison)\n",
        "avg_spacy_count = sum(r['spacy_count']   for r in results_comparison) / len(results_comparison)\n",
        "avg_nltk_time   = sum(r['nltk_time_ms']  for r in results_comparison) / len(results_comparison)\n",
        "avg_spacy_time  = sum(r['spacy_time_ms'] for r in results_comparison) / len(results_comparison)\n",
        "\n",
        "print(f'  NLTK  ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {avg_nltk_count:.1f} tokens/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°, ‡πÄ‡∏ß‡∏•‡∏≤ {avg_nltk_time:.1f} ms')\n",
        "print(f'  spaCy ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {avg_spacy_count:.1f} tokens/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°, ‡πÄ‡∏ß‡∏•‡∏≤ {avg_spacy_time:.1f} ms')\n",
        "print()\n",
        "print('üìå ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:')\n",
        "print('  ‚Ä¢ NLTK ‡πÉ‡∏ä‡πâ‡∏Å‡∏é regex-based ‡∏ï‡∏±‡∏î‡πÄ‡∏£‡πá‡∏ß ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà')\n",
        "print('  ‚Ä¢ spaCy ‡πÉ‡∏ä‡πâ statistical model ‡∏ï‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö POS/NER ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß')\n",
        "print('  ‚Ä¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ punct (‡πÄ‡∏ä‡πà‡∏ô \\'.\\'  \\',\\') ‚Äî spaCy ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô token ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "ZsxG_8eErYdT",
        "outputId": "6a944911-0a0e-4e0f-d4f3-4be2e23da166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á Tokenize ‡∏ó‡∏±‡πâ‡∏á Dataset...\n",
            "‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å yahoo_finance_phase1_final.csv ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: 466 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: ['headline', 'description', 'date', 'year', 'url', 'category', 'method', 'source', 'headline_clean', 'description_clean', 'nltk_tokens', 'nltk_token_count', 'spacy_tokens', 'spacy_token_count']\n",
            "\n",
            "--- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö description_clean (‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á) ---\n",
            "   description_clean ‡∏ß‡πà‡∏≤‡∏á : 0 ‡πÅ‡∏ñ‡∏ß (0.0%)\n",
            "   description_clean ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤: 466 ‡πÅ‡∏ñ‡∏ß\n",
            "\n",
            "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      headline_clean  \\\n",
              "0  3 no-brainer growth stocks to buy with 500 rig...   \n",
              "1  ai every tech revolution happens 'faster than ...   \n",
              "2  move over, annaly stock this unstoppable finan...   \n",
              "\n",
              "                                   description_clean  nltk_token_count  \\\n",
              "0  these growth stocks are big names, but they ca...                10   \n",
              "1  vice chair at citizens commercial bank, mark l...                11   \n",
              "2  main street capital pays a steadily rising reg...                14   \n",
              "\n",
              "   spacy_token_count  \n",
              "0                 12  \n",
              "1                 12  \n",
              "2                 14  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0ccc89f-7113-4905-a17c-a8d3a828911b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline_clean</th>\n",
              "      <th>description_clean</th>\n",
              "      <th>nltk_token_count</th>\n",
              "      <th>spacy_token_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3 no-brainer growth stocks to buy with 500 rig...</td>\n",
              "      <td>these growth stocks are big names, but they ca...</td>\n",
              "      <td>10</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ai every tech revolution happens 'faster than ...</td>\n",
              "      <td>vice chair at citizens commercial bank, mark l...</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>move over, annaly stock this unstoppable finan...</td>\n",
              "      <td>main street capital pays a steadily rising reg...</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0ccc89f-7113-4905-a17c-a8d3a828911b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0ccc89f-7113-4905-a17c-a8d3a828911b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0ccc89f-7113-4905-a17c-a8d3a828911b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_clean[['headline_clean', 'description_clean', 'nltk_token_count', 'spacy_token_count']]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"headline_clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"3 no-brainer growth stocks to buy with 500 right now\",\n          \"ai every tech revolution happens 'faster than the last one'\",\n          \"move over, annaly stock this unstoppable financial stock is a better buy today\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description_clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"these growth stocks are big names, but they can still produce big returns.\",\n          \"vice chair at citizens commercial bank, mark lehmann, speaks with market catalysts host julie hyman about his view on the speed at which ai is advancing and how the software sector is in turn responding. to watch more expert insights and analysis on the latest market action, check out more morning b\",\n          \"main street capital pays a steadily rising regular monthly dividend.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nltk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 10,\n        \"max\": 14,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10,\n          11,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spacy_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 12,\n        \"max\": 14,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          14,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# =====================================================\n",
        "# Apply Tokenization ‡∏ó‡∏±‡πâ‡∏á Dataset ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå nltk_tokens ‡πÅ‡∏•‡∏∞ spacy_tokens\n",
        "# =====================================================\n",
        "\n",
        "print('‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á Tokenize ‡∏ó‡∏±‡πâ‡∏á Dataset...')\n",
        "\n",
        "# NLTK tokenize ‡∏ó‡∏±‡πâ‡∏á dataset\n",
        "df_clean['nltk_tokens']  = df_clean['headline_clean'].apply(\n",
        "    lambda x: word_tokenize(x) if x else []\n",
        ")\n",
        "df_clean['nltk_token_count'] = df_clean['nltk_tokens'].apply(len)\n",
        "\n",
        "# spaCy tokenize ‡∏ó‡∏±‡πâ‡∏á dataset\n",
        "def spacy_tokenize(text):\n",
        "    if not text:\n",
        "        return []\n",
        "    doc = nlp(text)\n",
        "    return [t.text for t in doc if not t.is_space]\n",
        "\n",
        "df_clean['spacy_tokens']  = df_clean['headline_clean'].apply(spacy_tokenize)\n",
        "df_clean['spacy_token_count'] = df_clean['spacy_tokens'].apply(len)\n",
        "\n",
        "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á list ‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\n",
        "df_save = df_clean.copy()\n",
        "df_save['nltk_tokens']  = df_save['nltk_tokens'].apply(lambda x: ' | '.join(x))\n",
        "df_save['spacy_tokens'] = df_save['spacy_tokens'].apply(lambda x: ' | '.join(x))\n",
        "df_save.to_csv('yahoo_finance_phase1_final.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f'‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å yahoo_finance_phase1_final.csv ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')\n",
        "print(f'   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(df_clean)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "print(f'   ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {list(df_clean.columns)}')\n",
        "print(f'\\n--- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö description_clean (‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á) ---')\n",
        "empty_desc = (df_clean['description_clean'] == '').sum()\n",
        "print(f'   description_clean ‡∏ß‡πà‡∏≤‡∏á : {empty_desc} ‡πÅ‡∏ñ‡∏ß ({empty_desc/len(df_clean)*100:.1f}%)')\n",
        "print(f'   description_clean ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤: {len(df_clean)-empty_desc} ‡πÅ‡∏ñ‡∏ß')\n",
        "print(f'\\n--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å ---')\n",
        "df_clean[['headline_clean', 'description_clean', 'nltk_token_count', 'spacy_token_count']].head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eM1LNDi_sLZ",
        "outputId": "59673f81-6ba6-48ba-ff80-a4572731a003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checklist Phase 1 ‚Äî ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Phase 2 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á:\n",
            "=======================================================\n",
            "  ‚ùå ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ‚â• 500 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (466)\n",
            "  ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ 2025-2026\n",
            "  ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥\n",
            "  ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ headline ‡∏ß‡πà‡∏≤‡∏á\n",
            "  ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ description ‡∏ß‡πà‡∏≤‡∏á\n",
            "  ‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå nltk_tokens\n",
            "  ‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå spacy_tokens\n",
            "=======================================================\n",
            "\n",
            "‚ö†Ô∏è ‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏Å‡πà‡∏≠‡∏ô\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# Checklist ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Phase 2\n",
        "# =====================================================\n",
        "\n",
        "print('üîç Checklist Phase 1 ‚Äî ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Phase 2 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á:')\n",
        "print('='*55)\n",
        "\n",
        "checks = {\n",
        "    f'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ‚â• 500 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ({len(df_clean)})': len(df_clean) >= 500,\n",
        "    f'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ 2025-2026': df_clean['year'].isin([2025, 2026]).all(),\n",
        "    f'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥': df_clean.duplicated(subset=['headline']).sum() == 0,\n",
        "    f'‡πÑ‡∏°‡πà‡∏°‡∏µ headline ‡∏ß‡πà‡∏≤‡∏á': df_clean['headline_clean'].isna().sum() == 0,\n",
        "    f'‡πÑ‡∏°‡πà‡∏°‡∏µ description ‡∏ß‡πà‡∏≤‡∏á': (df_clean['description_clean'].str.strip() == '').sum() == 0,\n",
        "    f'‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå nltk_tokens': 'nltk_tokens' in df_clean.columns,\n",
        "    f'‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå spacy_tokens': 'spacy_tokens' in df_clean.columns,\n",
        "}\n",
        "\n",
        "all_pass = True\n",
        "for check, passed in checks.items():\n",
        "    icon = '‚úÖ' if passed else '‚ùå'\n",
        "    print(f'  {icon} {check}')\n",
        "    if not passed:\n",
        "        all_pass = False\n",
        "\n",
        "print('='*55)\n",
        "if all_pass:\n",
        "    print('\\n‚úÖ ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç! ‡πÑ‡∏ü‡∏•‡πå yahoo_finance_phase1_final.csv ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô')\n",
        "    print(\"   ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô Phase 2 ‡∏î‡πâ‡∏ß‡∏¢: pd.read_csv('yahoo_finance_phase1_final.csv')\")\n",
        "else:\n",
        "    print('\\n‚ö†Ô∏è ‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏Å‡πà‡∏≠‡∏ô')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtN5p5Wr_sLZ"
      },
      "source": [
        "---\n",
        "## üìã AI Audit Log ‚Äî ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Phase 5\n",
        "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVefSZtE_sLZ",
        "outputId": "73ead5d0-e126-47df-d90d-a3b19b7a229a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã AI Audit Log ‚Äî Web Scraping Section:\n",
            "              Task       Status                                                                                         ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç\n",
            "   RSS Feed Parser Pass w/ Edit AI ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏õ‡∏•‡∏á pubDate format ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD ‡πÄ‡∏û‡∏¥‡πà‡∏° strptime() ‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞ try/except ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error\n",
            "HTML Tag Selection         Fail        Class name ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πà‡∏≠‡∏¢ ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô find_all([\"h1\",\"h2\",\"h3\"]) ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ class ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô\n",
            " Duplicate Removal Pass w/ Edit       ‡πÄ‡∏û‡∏¥‡πà‡∏° seen_headlines set ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Scraping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á‡∏ã‡πâ‡∏≥‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Request ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n",
            "\n",
            "‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å AI Audit Log ‚Üí audit_log_scraping.csv\n"
          ]
        }
      ],
      "source": [
        "audit = pd.DataFrame([\n",
        "    {\n",
        "        'Task':         'RSS Feed Parser',\n",
        "        'Prompt':       'Write BeautifulSoup code to parse Yahoo Finance RSS XML feed',\n",
        "        'AI Output':    'soup.find_all(\"item\") with title and pubDate extraction',\n",
        "        'Status':       'Pass w/ Edit',\n",
        "        '‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç':    'AI ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏õ‡∏•‡∏á pubDate format ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD ‡πÄ‡∏û‡∏¥‡πà‡∏° strptime() ‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞ try/except ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error'\n",
        "    },\n",
        "    {\n",
        "        'Task':         'HTML Tag Selection',\n",
        "        'Prompt':       'Which HTML tags contain headlines on Yahoo Finance news page',\n",
        "        'AI Output':    'soup.find_all(\"h3\", class_=\"title\")',\n",
        "        'Status':       'Fail',\n",
        "        '‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç':    'Class name ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πà‡∏≠‡∏¢ ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô find_all([\"h1\",\"h2\",\"h3\"]) ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ class ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô'\n",
        "    },\n",
        "    {\n",
        "        'Task':         'Duplicate Removal',\n",
        "        'Prompt':       'Remove duplicate headlines across multiple scraped DataFrames',\n",
        "        'AI Output':    'df.drop_duplicates()',\n",
        "        'Status':       'Pass w/ Edit',\n",
        "        '‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç':    '‡πÄ‡∏û‡∏¥‡πà‡∏° seen_headlines set ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Scraping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á‡∏ã‡πâ‡∏≥‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Request ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô'\n",
        "    },\n",
        "])\n",
        "\n",
        "print('üìã AI Audit Log ‚Äî Web Scraping Section:')\n",
        "print(audit[['Task', 'Status', '‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç']].to_string(index=False))\n",
        "audit.to_csv('audit_log_scraping.csv', index=False, encoding='utf-8-sig')\n",
        "print('\\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å AI Audit Log ‚Üí audit_log_scraping.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
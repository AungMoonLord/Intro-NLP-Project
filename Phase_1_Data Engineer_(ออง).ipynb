{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 16:39:12,449 [INFO] üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° RSS Scraping ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ 700 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏à‡∏≤‡∏Å 37 feeds\n",
      "2026-02-18 16:39:12,455 [INFO]   [ 1/37] Yahoo Finance...\n",
      "2026-02-18 16:40:59,216 [INFO]          +48 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 48/700\n",
      "2026-02-18 16:40:59,216 [INFO]   [ 2/37] Yahoo Finance - S&P500...\n",
      "2026-02-18 16:41:42,390 [INFO]          +20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 68/700\n",
      "2026-02-18 16:41:42,390 [INFO]   [ 3/37] Yahoo Finance - Dow...\n",
      "2026-02-18 16:42:02,444 [INFO]          +10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 78/700\n",
      "2026-02-18 16:42:02,445 [INFO]   [ 4/37] Yahoo Finance - NASDAQ...\n",
      "2026-02-18 16:42:12,674 [INFO]          +5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 83/700\n",
      "2026-02-18 16:42:12,674 [INFO]   [ 5/37] Yahoo Finance - Gold...\n",
      "2026-02-18 16:43:38,174 [INFO]          +19 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 102/700\n",
      "2026-02-18 16:43:38,175 [INFO]   [ 6/37] Yahoo Finance - Oil...\n",
      "2026-02-18 16:45:22,121 [INFO]          +18 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 120/700\n",
      "2026-02-18 16:45:22,122 [INFO]   [ 7/37] Investing.com - Stock Market...\n",
      "2026-02-18 16:45:34,482 [INFO]          +1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 121/700\n",
      "2026-02-18 16:45:34,483 [INFO]   [ 8/37] Investing.com - Economy...\n",
      "2026-02-18 16:45:44,563 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 121/700\n",
      "2026-02-18 16:45:44,564 [INFO]   [ 9/37] Investing.com - Forex...\n",
      "2026-02-18 16:45:54,971 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 121/700\n",
      "2026-02-18 16:45:54,972 [INFO]   [10/37] Investing.com - Commodities...\n",
      "2026-02-18 16:45:58,387 [WARNING] [RSS WARN] Investing.com - Commodities: <unknown>:5:90: not well-formed (invalid token)\n",
      "2026-02-18 16:45:58,387 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 121/700\n",
      "2026-02-18 16:45:58,388 [INFO]   [11/37] Investing.com - Crypto...\n",
      "2026-02-18 16:46:09,092 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 121/700\n",
      "2026-02-18 16:46:09,093 [INFO]   [12/37] CNBC - Top News...\n",
      "2026-02-18 16:47:12,470 [INFO]          +30 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 151/700\n",
      "2026-02-18 16:47:12,470 [INFO]   [13/37] CNBC - Finance...\n",
      "2026-02-18 16:48:16,000 [INFO]          +28 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 179/700\n",
      "2026-02-18 16:48:16,000 [INFO]   [14/37] CNBC - Earnings...\n",
      "2026-02-18 16:49:20,400 [INFO]          +29 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 208/700\n",
      "2026-02-18 16:49:20,401 [INFO]   [15/37] CNBC - Markets...\n",
      "2026-02-18 16:50:14,540 [INFO]          +29 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 237/700\n",
      "2026-02-18 16:50:14,541 [INFO]   [16/37] CNBC - Economy...\n",
      "2026-02-18 16:50:14,845 [WARNING] [RSS WARN] CNBC - Economy: <unknown>:2:0: not well-formed (invalid token)\n",
      "2026-02-18 16:50:14,846 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 237/700\n",
      "2026-02-18 16:50:14,846 [INFO]   [17/37] CNBC - World Markets...\n",
      "2026-02-18 16:50:34,995 [INFO]          +9 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 246/700\n",
      "2026-02-18 16:50:34,995 [INFO]   [18/37] Reuters - Business...\n",
      "2026-02-18 16:50:35,079 [WARNING] [RSS WARN] Reuters - Business: <urlopen error [Errno 11001] getaddrinfo failed>\n",
      "2026-02-18 16:50:35,080 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 246/700\n",
      "2026-02-18 16:50:35,081 [INFO]   [19/37] Reuters - Markets...\n",
      "2026-02-18 16:50:35,096 [WARNING] [RSS WARN] Reuters - Markets: <urlopen error [Errno 11001] getaddrinfo failed>\n",
      "2026-02-18 16:50:35,096 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 246/700\n",
      "2026-02-18 16:50:35,096 [INFO]   [20/37] Reuters - US...\n",
      "2026-02-18 16:50:35,116 [WARNING] [RSS WARN] Reuters - US: <urlopen error [Errno 11001] getaddrinfo failed>\n",
      "2026-02-18 16:50:35,116 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 246/700\n",
      "2026-02-18 16:50:35,117 [INFO]   [21/37] AP - Business...\n",
      "2026-02-18 16:50:35,395 [WARNING] [RSS WARN] AP - Business: <unknown>:2:751: not well-formed (invalid token)\n",
      "2026-02-18 16:50:35,396 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 246/700\n",
      "2026-02-18 16:50:35,397 [INFO]   [22/37] AP - Economy...\n",
      "2026-02-18 16:50:35,549 [WARNING] [RSS WARN] AP - Economy: <unknown>:2:751: not well-formed (invalid token)\n",
      "2026-02-18 16:50:35,550 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 246/700\n",
      "2026-02-18 16:50:35,550 [INFO]   [23/37] AP - Finance...\n",
      "2026-02-18 16:50:35,704 [WARNING] [RSS WARN] AP - Finance: <unknown>:2:751: not well-formed (invalid token)\n",
      "2026-02-18 16:50:35,705 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 246/700\n",
      "2026-02-18 16:50:35,706 [INFO]   [24/37] Forbes - Business...\n",
      "2026-02-18 16:51:06,046 [INFO]          +25 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 271/700\n",
      "2026-02-18 16:51:06,046 [INFO]   [25/37] Forbes - Markets...\n",
      "2026-02-18 16:51:06,426 [WARNING] [RSS WARN] Forbes - Markets: <unknown>:26:439: not well-formed (invalid token)\n",
      "2026-02-18 16:51:06,427 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 271/700\n",
      "2026-02-18 16:51:06,427 [INFO]   [26/37] Forbes - Investing...\n",
      "2026-02-18 16:51:07,627 [WARNING] [RSS WARN] Forbes - Investing: <unknown>:26:439: not well-formed (invalid token)\n",
      "2026-02-18 16:51:07,628 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 271/700\n",
      "2026-02-18 16:51:07,628 [INFO]   [27/37] MarketWatch - Top Stories...\n",
      "2026-02-18 16:51:20,651 [INFO]          +9 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 280/700\n",
      "2026-02-18 16:51:20,651 [INFO]   [28/37] MarketWatch - Market Pulse...\n",
      "2026-02-18 16:51:54,986 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 280/700\n",
      "2026-02-18 16:51:54,986 [INFO]   [29/37] MarketWatch - Real Estate...\n",
      "2026-02-18 16:51:55,985 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 280/700\n",
      "2026-02-18 16:51:55,985 [INFO]   [30/37] Seeking Alpha - Market News...\n",
      "2026-02-18 16:52:07,173 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 280/700\n",
      "2026-02-18 16:52:07,173 [INFO]   [31/37] Seeking Alpha - Wall Street...\n",
      "2026-02-18 16:52:07,707 [WARNING] [RSS WARN] Seeking Alpha - Wall Street: <unknown>:44:59: not well-formed (invalid token)\n",
      "2026-02-18 16:52:07,707 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 280/700\n",
      "2026-02-18 16:52:07,707 [INFO]   [32/37] Barron's...\n",
      "2026-02-18 16:52:09,274 [WARNING] [RSS WARN] Barron's: <unknown>:3:2: syntax error\n",
      "2026-02-18 16:52:09,274 [INFO]          +0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 280/700\n",
      "2026-02-18 16:52:09,275 [INFO]   [33/37] The Economist - Finance...\n",
      "2026-02-18 16:58:08,300 [INFO]          +145 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 425/700\n",
      "2026-02-18 16:58:08,300 [INFO]   [34/37] The Economist - Business...\n",
      "2026-02-18 17:04:19,114 [INFO]          +145 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 570/700\n",
      "2026-02-18 17:04:19,115 [INFO]   [35/37] Motley Fool...\n",
      "2026-02-18 17:05:21,341 [INFO]          +44 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 614/700\n",
      "2026-02-18 17:05:21,342 [INFO]   [36/37] Nasdaq - Original...\n",
      "2026-02-18 17:06:12,320 [INFO]          +13 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 627/700\n",
      "2026-02-18 17:06:12,321 [INFO]   [37/37] Nasdaq - Markets...\n",
      "2026-02-18 17:06:35,628 [INFO]          +6 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° 633/700\n",
      "2026-02-18 17:06:35,629 [INFO] ‚úÖ ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ 633 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
      "2026-02-18 17:06:35,669 [INFO] üíæ raw_news.csv (633 rows)\n",
      "2026-02-18 17:06:38,593 [INFO] üßπ Preprocessing Pipeline...\n",
      "2026-02-18 17:06:38,594 [INFO]   [1/4] ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î text...\n",
      "2026-02-18 17:06:38,765 [INFO]   [2/4] ‡∏•‡∏ö article ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ: 261 rows ‡∏≠‡∏≠‡∏Å\n",
      "2026-02-18 17:06:38,766 [INFO]   [3/4] NLTK tokenization...\n",
      "2026-02-18 17:06:39,259 [INFO]   [4/4] spaCy tokenization...\n",
      "2026-02-18 17:06:59,855 [INFO] ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 372 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
      "2026-02-18 17:06:59,944 [INFO] üíæ processed_news.csv (372 rows)\n",
      "2026-02-18 17:07:03,856 [INFO] TensorFlow version 2.20.0 available.\n",
      "2026-02-18 17:07:05,996 [INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\aungl\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "2026-02-18 17:07:05,997 [INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\aungl\\.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "üìä TOKENIZATION COMPARISON\n",
      "=================================================================\n",
      "Sample: An Israeli Defense Forces reservist and a civilian have been indicted for allegedly using classified...\n",
      "\n",
      "üî§ NLTK (Word-level)\n",
      "   Tokens : 44\n",
      "   Sample : ['israeli', 'defense', 'forces', 'reservist', 'civilian', 'indicted', 'allegedly', 'using']\n",
      "\n",
      "üî§ spaCy (Word-level)\n",
      "   Tokens : 43\n",
      "   Sample : ['israeli', 'defense', 'forces', 'reservist', 'civilian', 'indict', 'allegedly', 'classified']\n",
      "\n",
      "üî§ GPT-2 BPE (Subword)\n",
      "   Tokens : 90\n",
      "   Sample : ['An', 'ƒ†Israeli', 'ƒ†Defense', 'ƒ†Forces', 'ƒ†reserv', 'ist', 'ƒ†and', 'ƒ†a']\n",
      "\n",
      "üí° spaCy ‡πÉ‡∏ä‡πâ lemmatization (running‚Üírun) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ vocab ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ NLTK. GPT-2 BPE ‡πÅ‡∏ö‡πà‡∏á token ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö subword ‡πÄ‡∏ä‡πà‡∏ô 'financial' ‚Üí ['fin','anc','ial'] ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏´‡∏≤‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Word-level.\n",
      "=================================================================\n",
      "\n",
      "üìà SUMMARY\n",
      "   Raw articles      : 633\n",
      "   After filtering   : 372\n",
      "   Yahoo Finance                      : 48\n",
      "   Motley Fool                        : 44\n",
      "   CNBC - Top News                    : 30\n",
      "   CNBC - Markets                     : 29\n",
      "   CNBC - Earnings                    : 29\n",
      "   CNBC - Finance                     : 28\n",
      "   The Economist - Business           : 23\n",
      "   Forbes - Business                  : 21\n",
      "   Yahoo Finance - S&P500             : 20\n",
      "   Yahoo Finance - Gold               : 19\n",
      "   Yahoo Finance - Oil                : 18\n",
      "   The Economist - Finance            : 14\n",
      "   Nasdaq - Original                  : 13\n",
      "   Yahoo Finance - Dow                : 10\n",
      "   CNBC - World Markets               : 9\n",
      "   Nasdaq - Markets                   : 6\n",
      "   Yahoo Finance - NASDAQ             : 5\n",
      "   MarketWatch - Top Stories          : 5\n",
      "   Investing.com - Stock Market       : 1\n",
      "   Output dir        : c:\\Users\\aungl\\Downloads\\NLP Project\\Version 2\\output_data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================\n",
    " Financial News Scraper v2 - RSS-Based (‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å block)\n",
    "=============================================================\n",
    " ‡πÉ‡∏ä‡πâ RSS Feed ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 401/403\n",
    " \n",
    " ‡πÅ‡∏´‡∏•‡πà‡∏á RSS ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:\n",
    "   1. Yahoo Finance RSS        - finance.yahoo.com\n",
    "   2. Investing.com RSS        - investing.com\n",
    "   3. CNBC RSS                 - cnbc.com\n",
    "   4. Financial Times RSS      - ft.com\n",
    "   5. Seeking Alpha RSS        - seekingalpha.com\n",
    "   6. Reuters RSS (Newsfeed)   - feeds.reuters.com\n",
    "   7. AP Business RSS          - apnews.com\n",
    "   8. WSJ RSS                  - wsj.com\n",
    "   9. Barron's RSS             - barrons.com\n",
    "  10. Forbes Finance RSS       - forbes.com\n",
    "\n",
    " Target: 500-1,000 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
    "=============================================================\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser          # pip install feedparser\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  RSS FEED LIST - ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô public feed ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å block\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "RSS_FEEDS = [\n",
    "    # ‚îÄ‚îÄ Yahoo Finance ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Yahoo Finance\", \"url\": \"https://finance.yahoo.com/news/rssindex\"},\n",
    "    {\"source\": \"Yahoo Finance - S&P500\", \"url\": \"https://finance.yahoo.com/rss/headline?s=^GSPC\"},\n",
    "    {\"source\": \"Yahoo Finance - Dow\",    \"url\": \"https://finance.yahoo.com/rss/headline?s=^DJI\"},\n",
    "    {\"source\": \"Yahoo Finance - NASDAQ\", \"url\": \"https://finance.yahoo.com/rss/headline?s=^IXIC\"},\n",
    "    {\"source\": \"Yahoo Finance - Gold\",   \"url\": \"https://finance.yahoo.com/rss/headline?s=GC=F\"},\n",
    "    {\"source\": \"Yahoo Finance - Oil\",    \"url\": \"https://finance.yahoo.com/rss/headline?s=CL=F\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ Investing.com ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Investing.com - Stock Market\", \"url\": \"https://www.investing.com/rss/news_285.rss\"},\n",
    "    {\"source\": \"Investing.com - Economy\",      \"url\": \"https://www.investing.com/rss/news_1.rss\"},\n",
    "    {\"source\": \"Investing.com - Forex\",        \"url\": \"https://www.investing.com/rss/news_25.rss\"},\n",
    "    {\"source\": \"Investing.com - Commodities\",  \"url\": \"https://www.investing.com/rss/news_8.rss\"},\n",
    "    {\"source\": \"Investing.com - Crypto\",       \"url\": \"https://www.investing.com/rss/news_301.rss\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ CNBC ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"CNBC - Top News\",     \"url\": \"https://www.cnbc.com/id/100003114/device/rss/rss.html\"},\n",
    "    {\"source\": \"CNBC - Finance\",      \"url\": \"https://www.cnbc.com/id/10000664/device/rss/rss.html\"},\n",
    "    {\"source\": \"CNBC - Earnings\",     \"url\": \"https://www.cnbc.com/id/15839135/device/rss/rss.html\"},\n",
    "    {\"source\": \"CNBC - Markets\",      \"url\": \"https://www.cnbc.com/id/20910258/device/rss/rss.html\"},\n",
    "    {\"source\": \"CNBC - Economy\",      \"url\": \"https://www.cnbc.com/id/20910274/device/rss/rss.html\"},\n",
    "    {\"source\": \"CNBC - World Markets\",\"url\": \"https://www.cnbc.com/id/15839069/device/rss/rss.html\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ Reuters (Newsfeed public RSS) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Reuters - Business\", \"url\": \"https://feeds.reuters.com/reuters/businessNews\"},\n",
    "    {\"source\": \"Reuters - Markets\",  \"url\": \"https://feeds.reuters.com/reuters/companyNews\"},\n",
    "    {\"source\": \"Reuters - US\",       \"url\": \"https://feeds.reuters.com/Reuters/domesticNews\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ AP News ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"AP - Business\",  \"url\": \"https://rsshub.app/apnews/topics/business\"},\n",
    "    {\"source\": \"AP - Economy\",   \"url\": \"https://rsshub.app/apnews/topics/economy\"},\n",
    "    {\"source\": \"AP - Finance\",   \"url\": \"https://rsshub.app/apnews/topics/financial-markets\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ Forbes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Forbes - Business\",   \"url\": \"https://www.forbes.com/business/feed/\"},\n",
    "    {\"source\": \"Forbes - Markets\",    \"url\": \"https://www.forbes.com/money/feed/\"},\n",
    "    {\"source\": \"Forbes - Investing\",  \"url\": \"https://www.forbes.com/investing/feed/\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ MarketWatch (RSS ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å block) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"MarketWatch - Top Stories\",  \"url\": \"https://feeds.content.dowjones.io/public/rss/mw_topstories\"},\n",
    "    {\"source\": \"MarketWatch - Market Pulse\", \"url\": \"https://feeds.content.dowjones.io/public/rss/mw_marketpulse\"},\n",
    "    {\"source\": \"MarketWatch - Real Estate\",  \"url\": \"https://feeds.content.dowjones.io/public/rss/mw_realestate\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ Seeking Alpha ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Seeking Alpha - Market News\", \"url\": \"https://seekingalpha.com/market_currents.xml\"},\n",
    "    {\"source\": \"Seeking Alpha - Wall Street\", \"url\": \"https://seekingalpha.com/feed/wall-street-breakfast\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ Barron's ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Barron's\", \"url\": \"https://www.barrons.com/xml/rss/3_7510.xml\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ The Economist ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"The Economist - Finance\", \"url\": \"https://www.economist.com/finance-and-economics/rss.xml\"},\n",
    "    {\"source\": \"The Economist - Business\",\"url\": \"https://www.economist.com/business/rss.xml\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ Motley Fool ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Motley Fool\", \"url\": \"https://www.fool.com/feeds/index.aspx\"},\n",
    "\n",
    "    # ‚îÄ‚îÄ Nasdaq ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    {\"source\": \"Nasdaq - Original\", \"url\": \"https://www.nasdaq.com/feed/rssoutbound?category=Original+Articles\"},\n",
    "    {\"source\": \"Nasdaq - Markets\",  \"url\": \"https://www.nasdaq.com/feed/rssoutbound?category=Markets\"},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d841e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  SECTION 1.1 ‚Äì DATA COLLECTION (RSS-based)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class RSSFinancialScraper:\n",
    "\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"application/rss+xml, application/xml, text/xml, */*\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, target: int = 700, fetch_full_article: bool = True,\n",
    "                 delay: tuple = (0.5, 1.5)):\n",
    "        self.target = target\n",
    "        self.fetch_full = fetch_full_article   # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å URL ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà summary\n",
    "        self.delay = delay\n",
    "        self.articles: list[dict] = []\n",
    "        self.seen_titles: set[str] = set()\n",
    "\n",
    "    # ‚îÄ‚îÄ Utilities ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "    def _sleep(self):\n",
    "        time.sleep(random.uniform(*self.delay))\n",
    "\n",
    "    def _get_html(self, url: str) -> BeautifulSoup | None:\n",
    "        \"\"\"‡∏î‡∏∂‡∏á HTML page ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö full article.\"\"\"\n",
    "        try:\n",
    "            self._sleep()\n",
    "            r = requests.get(url, headers=self.HEADERS, timeout=12)\n",
    "            r.raise_for_status()\n",
    "            return BeautifulSoup(r.text, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"[HTML ERROR] {url[:60]} ‚Üí {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_full_content(self, url: str) -> str:\n",
    "        \"\"\"‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å article URL.\"\"\"\n",
    "        soup = self._get_html(url)\n",
    "        if not soup:\n",
    "            return \"\"\n",
    "        # ‡∏•‡∏≠‡∏á selector ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n",
    "        for selector in [\n",
    "            {\"tag\": \"article\"},\n",
    "            {\"tag\": \"div\", \"class_\": re.compile(r\"article.body|story.body|content.body|post.body\", re.I)},\n",
    "            {\"tag\": \"div\", \"id\": re.compile(r\"article|content|main\", re.I)},\n",
    "        ]:\n",
    "            tag = selector.pop(\"tag\")\n",
    "            el = soup.find(tag, **selector)\n",
    "            if el:\n",
    "                paras = el.find_all(\"p\")\n",
    "                text = \" \".join(p.get_text(\" \", strip=True) for p in paras)\n",
    "                if len(text.split()) > 30:\n",
    "                    return text\n",
    "        # fallback: ‡∏î‡∏∂‡∏á <p> ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "        paras = soup.find_all(\"p\")\n",
    "        return \" \".join(p.get_text(\" \", strip=True) for p in paras)\n",
    "\n",
    "    def _parse_rss(self, feed_url: str, source: str) -> int:\n",
    "        \"\"\"\n",
    "        Parse RSS feed ‡∏î‡πâ‡∏ß‡∏¢ feedparser ‡πÅ‡∏•‡∏∞ requests\n",
    "        ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô article ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ\n",
    "        \"\"\"\n",
    "        added = 0\n",
    "        try:\n",
    "            # ‡πÉ‡∏ä‡πâ feedparser (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Atom + RSS ‡∏ó‡∏∏‡∏Å version)\n",
    "            feed = feedparser.parse(feed_url, agent=self.HEADERS[\"User-Agent\"])\n",
    "\n",
    "            if feed.bozo and not feed.entries:\n",
    "                # feedparser ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á bozo=True ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏î‡πâ entries\n",
    "                logger.warning(f\"[RSS WARN] {source}: {getattr(feed, 'bozo_exception', 'unknown')}\")\n",
    "                return 0\n",
    "\n",
    "            for entry in feed.entries:\n",
    "                if len(self.articles) >= self.target:\n",
    "                    break\n",
    "\n",
    "                # ‚îÄ‚îÄ Title ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                title = getattr(entry, \"title\", \"\").strip()\n",
    "                if not title or title in self.seen_titles:\n",
    "                    continue\n",
    "                self.seen_titles.add(title)\n",
    "\n",
    "                # ‚îÄ‚îÄ URL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                url = getattr(entry, \"link\", \"\")\n",
    "\n",
    "                # ‚îÄ‚îÄ Content / Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: content > summary > description\n",
    "                content = \"\"\n",
    "                if hasattr(entry, \"content\") and entry.content:\n",
    "                    content = entry.content[0].get(\"value\", \"\")\n",
    "                if not content and hasattr(entry, \"summary\"):\n",
    "                    content = entry.summary\n",
    "                if not content and hasattr(entry, \"description\"):\n",
    "                    content = entry.description\n",
    "\n",
    "                # ‡∏•‡∏ö HTML ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å summary ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ tags\n",
    "                content = BeautifulSoup(content, \"html.parser\").get_text(\" \", strip=True)\n",
    "\n",
    "                # ‚îÄ‚îÄ Optionally fetch full article ‚îÄ‚îÄ\n",
    "                if self.fetch_full and url and len(content.split()) < 80:\n",
    "                    full = self._extract_full_content(url)\n",
    "                    if full:\n",
    "                        content = full\n",
    "\n",
    "                # ‚îÄ‚îÄ Published date ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                pub = \"\"\n",
    "                if hasattr(entry, \"published\"):\n",
    "                    pub = entry.published\n",
    "                elif hasattr(entry, \"updated\"):\n",
    "                    pub = entry.updated\n",
    "\n",
    "                if not content or len(content.split()) < 10:\n",
    "                    continue\n",
    "\n",
    "                self.articles.append({\n",
    "                    \"id\": len(self.articles) + 1,\n",
    "                    \"source\": source,\n",
    "                    \"title\": title,\n",
    "                    \"content\": content,\n",
    "                    \"url\": url,\n",
    "                    \"published\": pub,\n",
    "                    \"scraped_at\": datetime.now().isoformat(),\n",
    "                })\n",
    "                added += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[RSS ERROR] {source}: {e}\")\n",
    "\n",
    "        return added\n",
    "\n",
    "    # ‚îÄ‚îÄ Main Collection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "    def collect(self) -> pd.DataFrame:\n",
    "        logger.info(f\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° RSS Scraping ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ {self.target} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏à‡∏≤‡∏Å {len(RSS_FEEDS)} feeds\")\n",
    "\n",
    "        for i, feed in enumerate(RSS_FEEDS):\n",
    "            if len(self.articles) >= self.target:\n",
    "                break\n",
    "            source = feed[\"source\"]\n",
    "            url = feed[\"url\"]\n",
    "            logger.info(f\"  [{i+1:>2}/{len(RSS_FEEDS)}] {source}...\")\n",
    "            n = self._parse_rss(url, source)\n",
    "            logger.info(f\"         +{n} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏£‡∏ß‡∏° {len(self.articles)}/{self.target}\")\n",
    "\n",
    "        logger.info(f\"‚úÖ ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ {len(self.articles)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
    "        return pd.DataFrame(self.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  SECTION 1.2 ‚Äì PREPROCESSING PIPELINE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    1.2.1 ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î: HTML tags, URLs, Emojis, whitespace\n",
    "    1.2.2 Tokenization Comparison: NLTK vs spaCy vs Subword BPE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._init_nltk()\n",
    "        self._init_spacy()\n",
    "\n",
    "    def _init_nltk(self):\n",
    "        import nltk\n",
    "        for pkg in [\"punkt\", \"punkt_tab\", \"stopwords\", \"wordnet\"]:\n",
    "            nltk.download(pkg, quiet=True)\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        from nltk.corpus import stopwords\n",
    "        self.nltk_word_tokenize = word_tokenize\n",
    "        self.stopwords_en = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def _init_spacy(self):\n",
    "        try:\n",
    "            import spacy\n",
    "            try:\n",
    "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            except OSError:\n",
    "                os.system(\"python -m spacy download en_core_web_sm\")\n",
    "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"spaCy unavailable: {e}\")\n",
    "            self.nlp = None\n",
    "\n",
    "    # ‚îÄ‚îÄ 1.2.1 Cleaning ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_html(text: str) -> str:\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_urls(text: str) -> str:\n",
    "        text = re.sub(r\"http[s]?://\\S+\", \" \", text)\n",
    "        text = re.sub(r\"www\\.\\S+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_emojis(text: str) -> str:\n",
    "        emoji_re = re.compile(\n",
    "            \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\"\n",
    "            \"\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\"\n",
    "            \"\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        return emoji_re.sub(\" \", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_whitespace(text: str) -> str:\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s.,!?;:'\\\"-]\", \" \", text)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    def clean(self, text: str) -> str:\n",
    "        \"\"\"Pipeline: HTML ‚Üí URL ‚Üí Emoji ‚Üí normalize\"\"\"\n",
    "        text = self.remove_html(text)\n",
    "        text = self.remove_urls(text)\n",
    "        text = self.remove_emojis(text)\n",
    "        text = self.normalize_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    # ‚îÄ‚îÄ 1.2.2 Tokenizers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "    def tok_nltk(self, text: str) -> list[str]:\n",
    "        \"\"\"Word-level: NLTK word_tokenize + stopword removal\"\"\"\n",
    "        tokens = self.nltk_word_tokenize(text.lower())\n",
    "        return [t for t in tokens if t.isalpha() and t not in self.stopwords_en]\n",
    "\n",
    "    def tok_spacy(self, text: str) -> list[str]:\n",
    "        \"\"\"Word-level: spaCy lemmatize + stopword removal\"\"\"\n",
    "        if not self.nlp:\n",
    "            return []\n",
    "        doc = self.nlp(text[:100_000])\n",
    "        return [tok.lemma_.lower() for tok in doc\n",
    "                if tok.is_alpha and not tok.is_stop and len(tok.text) > 1]\n",
    "\n",
    "    def tok_subword(self, text: str) -> list[str]:\n",
    "        \"\"\"Subword BPE ‡∏î‡πâ‡∏ß‡∏¢ GPT-2 tokenizer (transformers)\"\"\"\n",
    "        try:\n",
    "            from transformers import GPT2Tokenizer\n",
    "            tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "            return tok.tokenize(text[:512])\n",
    "        except ImportError:\n",
    "            # fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á transformers\n",
    "            return re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text).lower().split()\n",
    "\n",
    "    # ‚îÄ‚îÄ Comparison Report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "    def compare(self, text: str) -> dict:\n",
    "        \"\"\"‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö NLTK vs spaCy (Word-level) vs GPT-2 BPE (Subword-level)\"\"\"\n",
    "        sample = text[:500]\n",
    "        n_toks = self.tok_nltk(sample)\n",
    "        s_toks = self.tok_spacy(sample)\n",
    "        b_toks = self.tok_subword(sample)\n",
    "\n",
    "        n_vocab = set(n_toks)\n",
    "        s_vocab = set(s_toks)\n",
    "\n",
    "        return {\n",
    "            \"sample_text\": sample,\n",
    "            \"NLTK_word_level\": {\n",
    "                \"method\": \"word_tokenize + stopword removal\",\n",
    "                \"token_count\": len(n_toks),\n",
    "                \"unique_vocab\": len(n_vocab),\n",
    "                \"sample\": n_toks[:15],\n",
    "            },\n",
    "            \"spaCy_word_level\": {\n",
    "                \"method\": \"spaCy lemmatize + stopword removal\",\n",
    "                \"token_count\": len(s_toks),\n",
    "                \"unique_vocab\": len(s_vocab),\n",
    "                \"sample\": s_toks[:15],\n",
    "            },\n",
    "            \"GPT2_subword_BPE\": {\n",
    "                \"method\": \"GPT-2 Byte-Pair Encoding (Subword-level)\",\n",
    "                \"token_count\": len(b_toks),\n",
    "                \"sample\": b_toks[:15],\n",
    "            },\n",
    "            \"analysis\": {\n",
    "                \"overlap_nltk_spacy\": len(n_vocab & s_vocab),\n",
    "                \"nltk_only\": len(n_vocab - s_vocab),\n",
    "                \"spacy_only\": len(s_vocab - n_vocab),\n",
    "                \"insight\": (\n",
    "                    \"spaCy ‡πÉ‡∏ä‡πâ lemmatization (running‚Üírun) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ vocab ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ NLTK. \"\n",
    "                    \"GPT-2 BPE ‡πÅ‡∏ö‡πà‡∏á token ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö subword ‡πÄ‡∏ä‡πà‡∏ô 'financial' ‚Üí ['fin','anc','ial'] \"\n",
    "                    \"‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏´‡∏≤‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Word-level.\"\n",
    "                ),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # ‚îÄ‚îÄ Run Pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "    def run(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(\"üßπ Preprocessing Pipeline...\")\n",
    "\n",
    "        # Step 1: clean\n",
    "        logger.info(\"  [1/4] ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î text...\")\n",
    "        df[\"title_clean\"]   = df[\"title\"].fillna(\"\").apply(self.clean)\n",
    "        df[\"content_clean\"] = df[\"content\"].fillna(\"\").apply(self.clean)\n",
    "        df[\"word_count\"]    = df[\"content_clean\"].str.split().str.len()\n",
    "\n",
    "        # Step 2: filter short articles\n",
    "        before = len(df)\n",
    "        df = df[df[\"word_count\"] >= 20].reset_index(drop=True)\n",
    "        logger.info(f\"  [2/4] ‡∏•‡∏ö article ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ: {before - len(df)} rows ‡∏≠‡∏≠‡∏Å\")\n",
    "\n",
    "        # Step 3: NLTK tokenize\n",
    "        logger.info(\"  [3/4] NLTK tokenization...\")\n",
    "        df[\"tokens_nltk\"]  = df[\"content_clean\"].apply(lambda x: self.tok_nltk(x[:3000]))\n",
    "        df[\"count_nltk\"]   = df[\"tokens_nltk\"].apply(len)\n",
    "\n",
    "        # Step 4: spaCy tokenize\n",
    "        logger.info(\"  [4/4] spaCy tokenization...\")\n",
    "        df[\"tokens_spacy\"] = df[\"content_clean\"].apply(lambda x: self.tok_spacy(x[:3000]))\n",
    "        df[\"count_spacy\"]  = df[\"tokens_spacy\"].apply(len)\n",
    "\n",
    "        logger.info(f\"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5feb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  MAIN\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def main():\n",
    "    OUT = \"output_data\"\n",
    "    os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 1.1 Collect ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    scraper = RSSFinancialScraper(\n",
    "        target=700,\n",
    "        fetch_full_article=True,   # True = ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏° (‡∏ä‡πâ‡∏≤‡∏•‡∏á‡∏ô‡∏¥‡∏î)\n",
    "        delay=(0.5, 1.5),\n",
    "    )\n",
    "    raw_df = scraper.collect()\n",
    "    raw_df.to_csv(f\"{OUT}/raw_news.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    logger.info(f\"üíæ raw_news.csv ({len(raw_df)} rows)\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ 1.2 Preprocess ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    pipeline = PreprocessingPipeline()\n",
    "    proc_df = pipeline.run(raw_df.copy())\n",
    "\n",
    "    # serialize token lists ‡πÄ‡∏õ‡πá‡∏ô JSON string ‡∏Å‡πà‡∏≠‡∏ô save\n",
    "    proc_df[\"tokens_nltk\"]  = proc_df[\"tokens_nltk\"].apply(json.dumps)\n",
    "    proc_df[\"tokens_spacy\"] = proc_df[\"tokens_spacy\"].apply(json.dumps)\n",
    "    proc_df.to_csv(f\"{OUT}/processed_news.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    logger.info(f\"üíæ processed_news.csv ({len(proc_df)} rows)\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Tokenization Comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if not proc_df.empty:\n",
    "        sample_text = raw_df[\"content\"].iloc[0]\n",
    "        report = pipeline.compare(sample_text)\n",
    "        with open(f\"{OUT}/tokenization_comparison.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 65)\n",
    "        print(\"üìä TOKENIZATION COMPARISON\")\n",
    "        print(\"=\" * 65)\n",
    "        print(f\"Sample: {report['sample_text'][:100]}...\\n\")\n",
    "        for name, key in [(\"NLTK (Word-level)\", \"NLTK_word_level\"),\n",
    "                          (\"spaCy (Word-level)\", \"spaCy_word_level\"),\n",
    "                          (\"GPT-2 BPE (Subword)\", \"GPT2_subword_BPE\")]:\n",
    "            r = report[key]\n",
    "            print(f\"üî§ {name}\")\n",
    "            print(f\"   Tokens : {r['token_count']}\")\n",
    "            print(f\"   Sample : {r['sample'][:8]}\")\n",
    "            print()\n",
    "        print(f\"üí° {report['analysis']['insight']}\")\n",
    "        print(\"=\" * 65)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    print(f\"\\nüìà SUMMARY\")\n",
    "    print(f\"   Raw articles      : {len(raw_df)}\")\n",
    "    print(f\"   After filtering   : {len(proc_df)}\")\n",
    "    if \"source\" in proc_df.columns:\n",
    "        src_counts = proc_df[\"source\"].value_counts()\n",
    "        for src, cnt in src_counts.items():\n",
    "            print(f\"   {src:<35}: {cnt}\")\n",
    "    print(f\"   Output dir        : {os.path.abspath(OUT)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-ubr4uKtSGm"
      },
      "source": [
        "# üì∞ Phase 1 ‚Äî Financial News Scraper & Preprocessing Pipeline\n",
        "### Merged Notebook: Yahoo Finance (RSS + HTML) + Multi-Source RSS\n",
        "\n",
        "**‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**\n",
        "- **Yahoo Finance** ‚Äî RSS Feed (19 feeds) + HTML Scraping (8 ‡∏´‡∏ô‡πâ‡∏≤)\n",
        "- **Multi-Source RSS** ‚Äî Investing.com, CNBC, Reuters, AP, Forbes, MarketWatch, Seeking Alpha, The Economist, Nasdaq\n",
        "\n",
        "**Pipeline:**\n",
        "1. **1.1** Data Collection ‚Üí ‡∏£‡∏ß‡∏° raw articles ‚â• 500 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏õ‡∏µ 2025‚Äì2026)\n",
        "2. **1.2** Cleaning ‚Üí ‡∏•‡∏ö HTML / URL / Emoji / ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏© ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å `clean_text_(Web_Scraping).csv`\n",
        "3. **1.2.5** Merge Kaggle Dataset ‚Üí ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö dataset ‡∏à‡∏≤‡∏Å Kaggle ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å `complete_dataset.csv`\n",
        "4. **1.3** Tokenization Comparison ‚Üí NLTK vs spaCy (‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ö‡∏ô `complete_dataset.csv`)"
      ],
      "id": "D-ubr4uKtSGm"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLXRTXTwtSGr",
        "outputId": "9ef44ef9-9e8b-4fea-fc8d-d15ad01f1d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  INSTALL DEPENDENCIES\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "!pip install requests beautifulsoup4 pandas lxml feedparser nltk spacy -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "print('‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')"
      ],
      "id": "oLXRTXTwtSGr"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uHWcbpYtSGt",
        "outputId": "675e2854-fd26-4e90-df79-f52c795d02b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Import ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à | ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î: 2026-02-19\n",
            "üìÅ Output dir : /content/output_data\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  IMPORTS & GLOBAL CONFIG\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "for pkg in ['punkt', 'punkt_tab', 'stopwords', 'wordnet']:\n",
        "    nltk.download(pkg, quiet=True)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    os.system('python -m spacy download en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "TODAY   = datetime.now().strftime('%Y-%m-%d')\n",
        "OUT_DIR = 'output_data'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå output (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)\n",
        "WEB_CSV      = f'{OUT_DIR}/clean_text_(Web_Scraping).csv'\n",
        "COMPLETE_CSV = f'{OUT_DIR}/complete_dataset.csv'\n",
        "\n",
        "# Browser-like headers (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 403)\n",
        "HEADERS = {\n",
        "    'User-Agent': (\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "        'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "        'Chrome/120.0.0.0 Safari/537.36'\n",
        "    ),\n",
        "    'Accept':          'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Referer':         'https://finance.yahoo.com/',\n",
        "}\n",
        "\n",
        "print(f'‚úÖ Import ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à | ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î: {TODAY}')\n",
        "print(f'üìÅ Output dir : {os.path.abspath(OUT_DIR)}')"
      ],
      "id": "_uHWcbpYtSGt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbDec8AUtSGu"
      },
      "source": [
        "## Section 1.1 ‚Äî Data Collection"
      ],
      "id": "TbDec8AUtSGu"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLtJyg9XtSGv",
        "outputId": "b50ff337-7cf8-40a7-9a32-3fbeab2ea329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° Yahoo Finance RSS Scraping...\n",
            "=======================================================\n",
            "  top_stories      ‚úÖ 46 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  latest_news      ‚úÖ 46 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  sp500            ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  nasdaq           ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  dowjones         ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  apple            ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  microsoft        ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  nvidia           ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  google           ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  meta             ‚úÖ 19 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  tesla            ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  amazon           ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  jpmorgan         ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  bank_america     ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  goldman          ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  bitcoin          ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  ethereum         ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  gold             ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  oil              ‚úÖ 20 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "üìä Yahoo RSS ‡∏£‡∏ß‡∏°: 431 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  1.1.1  YAHOO FINANCE ‚Äî RSS FEED SCRAPING\n",
        "#  ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ 100%, ‡πÑ‡∏î‡πâ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á, ‡πÑ‡∏î‡πâ description\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "YAHOO_RSS_FEEDS = {\n",
        "    # ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n",
        "    'top_stories':  'https://finance.yahoo.com/news/rssindex',\n",
        "    'latest_news':  'https://finance.yahoo.com/rss/topstories',\n",
        "    # ‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ï‡∏•‡∏≤‡∏î\n",
        "    'sp500':        'https://finance.yahoo.com/rss/headline?s=^GSPC',\n",
        "    'nasdaq':       'https://finance.yahoo.com/rss/headline?s=^IXIC',\n",
        "    'dowjones':     'https://finance.yahoo.com/rss/headline?s=^DJI',\n",
        "    # ‡∏´‡∏∏‡πâ‡∏ô Tech\n",
        "    'apple':        'https://finance.yahoo.com/rss/headline?s=AAPL',\n",
        "    'microsoft':    'https://finance.yahoo.com/rss/headline?s=MSFT',\n",
        "    'nvidia':       'https://finance.yahoo.com/rss/headline?s=NVDA',\n",
        "    'google':       'https://finance.yahoo.com/rss/headline?s=GOOGL',\n",
        "    'meta':         'https://finance.yahoo.com/rss/headline?s=META',\n",
        "    'tesla':        'https://finance.yahoo.com/rss/headline?s=TSLA',\n",
        "    'amazon':       'https://finance.yahoo.com/rss/headline?s=AMZN',\n",
        "    # Banking\n",
        "    'jpmorgan':     'https://finance.yahoo.com/rss/headline?s=JPM',\n",
        "    'bank_america': 'https://finance.yahoo.com/rss/headline?s=BAC',\n",
        "    'goldman':      'https://finance.yahoo.com/rss/headline?s=GS',\n",
        "    # ‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏≠‡∏∑‡πà‡∏ô\n",
        "    'bitcoin':      'https://finance.yahoo.com/rss/headline?s=BTC-USD',\n",
        "    'ethereum':     'https://finance.yahoo.com/rss/headline?s=ETH-USD',\n",
        "    'gold':         'https://finance.yahoo.com/rss/headline?s=GC=F',\n",
        "    'oil':          'https://finance.yahoo.com/rss/headline?s=CL=F',\n",
        "}\n",
        "\n",
        "\n",
        "def scrape_yahoo_rss(feed_url: str, category: str) -> list:\n",
        "    \"\"\"‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Yahoo Finance RSS ‚Äî ‡∏Ñ‡∏∑‡∏ô list of dict\"\"\"\n",
        "    try:\n",
        "        time.sleep(random.uniform(1.0, 2.0))\n",
        "        resp = requests.get(feed_url, headers=HEADERS, timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            return []\n",
        "        soup    = BeautifulSoup(resp.text, 'xml')\n",
        "        results = []\n",
        "        for item in soup.find_all('item'):\n",
        "            title    = item.find('title')\n",
        "            pub_date = item.find('pubDate')\n",
        "            desc     = item.find('description')\n",
        "            if not title:\n",
        "                continue\n",
        "            headline = title.get_text(strip=True)\n",
        "            if len(headline) < 20:\n",
        "                continue\n",
        "            raw_date = pub_date.get_text(strip=True) if pub_date else TODAY\n",
        "            try:\n",
        "                dt         = datetime.strptime(raw_date, '%a, %d %b %Y %H:%M:%S %z')\n",
        "                clean_date = dt.strftime('%Y-%m-%d')\n",
        "                year       = dt.year\n",
        "            except:\n",
        "                clean_date, year = TODAY, 2025\n",
        "            content = ''\n",
        "            if desc:\n",
        "                content = BeautifulSoup(desc.get_text(), 'html.parser').get_text(strip=True)[:800]\n",
        "            results.append({\n",
        "                'title':    headline,\n",
        "                'content':  content,\n",
        "                'date':     clean_date,\n",
        "                'year':     year,\n",
        "                'source':   'Yahoo Finance',\n",
        "                'category': category,\n",
        "            })\n",
        "        return results\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "print('üì° Yahoo Finance RSS Scraping...')\n",
        "print('=' * 55)\n",
        "yahoo_rss_rows = []\n",
        "for cat, url in YAHOO_RSS_FEEDS.items():\n",
        "    rows   = scrape_yahoo_rss(url, cat)\n",
        "    yahoo_rss_rows.extend(rows)\n",
        "    status = f'‚úÖ {len(rows)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£' if rows else '‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£'\n",
        "    print(f'  {cat:<16} {status}')\n",
        "\n",
        "df_yahoo_rss = pd.DataFrame(yahoo_rss_rows) if yahoo_rss_rows else pd.DataFrame()\n",
        "print(f'\\nüìä Yahoo RSS ‡∏£‡∏ß‡∏°: {len(df_yahoo_rss)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')"
      ],
      "id": "gLtJyg9XtSGv"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxWRechktSGw",
        "outputId": "9abf38a8-ad1b-403a-9b88-bbe7820cffd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Yahoo Finance HTML Scraping...\n",
            "=======================================================\n",
            "  news_main       ‚úÖ 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  markets         ‚úÖ 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  stocks          ‚úÖ 44 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  crypto          ‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  economy         ‚úÖ 49 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  earnings        ‚úÖ 52 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  tech            ‚úÖ 53 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "  real_estate     ‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "üìä Yahoo HTML ‡∏£‡∏ß‡∏°: 200 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  1.1.2  YAHOO FINANCE ‚Äî HTML SCRAPING (‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "YAHOO_HTML_URLS = {\n",
        "    'news_main':   'https://finance.yahoo.com/news/',\n",
        "    'markets':     'https://finance.yahoo.com/markets/',\n",
        "    'stocks':      'https://finance.yahoo.com/topic/stock-market-news/',\n",
        "    'crypto':      'https://finance.yahoo.com/crypto/',\n",
        "    'economy':     'https://finance.yahoo.com/topic/economic-news/',\n",
        "    'earnings':    'https://finance.yahoo.com/topic/earnings/',\n",
        "    'tech':        'https://finance.yahoo.com/topic/tech/',\n",
        "    'real_estate': 'https://finance.yahoo.com/topic/real-estate/',\n",
        "}\n",
        "\n",
        "\n",
        "def scrape_yahoo_html(url: str, category: str, seen: set) -> list:\n",
        "    \"\"\"Scrape headline ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ HTML Yahoo Finance\"\"\"\n",
        "    try:\n",
        "        time.sleep(random.uniform(2.0, 3.5))\n",
        "        resp = requests.get(url, headers=HEADERS, timeout=15)\n",
        "        if resp.status_code != 200:\n",
        "            return []\n",
        "        soup    = BeautifulSoup(resp.text, 'html.parser')\n",
        "        results = []\n",
        "        for tag in soup.find_all(['h1', 'h2', 'h3']):\n",
        "            text = tag.get_text(strip=True)\n",
        "            if len(text) <= 25 or text in seen:\n",
        "                continue\n",
        "            seen.add(text)\n",
        "            parent  = tag.parent\n",
        "            p_tag   = parent.find('p') if parent else None\n",
        "            content = p_tag.get_text(strip=True)[:800] if p_tag else text\n",
        "            results.append({\n",
        "                'title':    text,\n",
        "                'content':  content,\n",
        "                'date':     TODAY,\n",
        "                'year':     2025,\n",
        "                'source':   'Yahoo Finance',\n",
        "                'category': category,\n",
        "            })\n",
        "        return results\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "print('üåê Yahoo Finance HTML Scraping...')\n",
        "print('=' * 55)\n",
        "seen_yahoo      = set(df_yahoo_rss['title'].tolist() if not df_yahoo_rss.empty else [])\n",
        "yahoo_html_rows = []\n",
        "for cat, url in YAHOO_HTML_URLS.items():\n",
        "    rows   = scrape_yahoo_html(url, cat, seen_yahoo)\n",
        "    yahoo_html_rows.extend(rows)\n",
        "    status = f'‚úÖ {len(rows)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£' if rows else '‚ö†Ô∏è  0 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£'\n",
        "    print(f'  {cat:<15} {status}')\n",
        "\n",
        "df_yahoo_html = pd.DataFrame(yahoo_html_rows) if yahoo_html_rows else pd.DataFrame()\n",
        "print(f'\\nüìä Yahoo HTML ‡∏£‡∏ß‡∏°: {len(df_yahoo_html)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')"
      ],
      "id": "XxWRechktSGw"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-30dM2j2tSGw",
        "outputId": "3edaee13-48f8-4b57-d54d-67f43e070e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Multi-Source RSS ‡∏£‡∏ß‡∏°: 489 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  1.1.3  MULTI-SOURCE RSS SCRAPING\n",
        "#  ‡πÅ‡∏´‡∏•‡πà‡∏á: Investing.com, CNBC, Reuters, AP, Forbes,\n",
        "#          MarketWatch, Seeking Alpha, The Economist, Nasdaq\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "MULTI_RSS_FEEDS = [\n",
        "    # Investing.com\n",
        "    {'source': 'Investing.com - Stock Market', 'url': 'https://www.investing.com/rss/news_285.rss'},\n",
        "    {'source': 'Investing.com - Economy',      'url': 'https://www.investing.com/rss/news_1.rss'},\n",
        "    {'source': 'Investing.com - Forex',        'url': 'https://www.investing.com/rss/news_25.rss'},\n",
        "    {'source': 'Investing.com - Commodities',  'url': 'https://www.investing.com/rss/news_8.rss'},\n",
        "    {'source': 'Investing.com - Crypto',       'url': 'https://www.investing.com/rss/news_301.rss'},\n",
        "    # CNBC\n",
        "    {'source': 'CNBC - Top News',      'url': 'https://www.cnbc.com/id/100003114/device/rss/rss.html'},\n",
        "    {'source': 'CNBC - Finance',       'url': 'https://www.cnbc.com/id/10000664/device/rss/rss.html'},\n",
        "    {'source': 'CNBC - Earnings',      'url': 'https://www.cnbc.com/id/15839135/device/rss/rss.html'},\n",
        "    {'source': 'CNBC - Markets',       'url': 'https://www.cnbc.com/id/20910258/device/rss/rss.html'},\n",
        "    {'source': 'CNBC - Economy',       'url': 'https://www.cnbc.com/id/20910274/device/rss/rss.html'},\n",
        "    {'source': 'CNBC - World Markets', 'url': 'https://www.cnbc.com/id/15839069/device/rss/rss.html'},\n",
        "    # Reuters\n",
        "    {'source': 'Reuters - Business', 'url': 'https://feeds.reuters.com/reuters/businessNews'},\n",
        "    {'source': 'Reuters - Markets',  'url': 'https://feeds.reuters.com/reuters/companyNews'},\n",
        "    {'source': 'Reuters - US',       'url': 'https://feeds.reuters.com/Reuters/domesticNews'},\n",
        "    # AP News\n",
        "    {'source': 'AP - Business', 'url': 'https://rsshub.app/apnews/topics/business'},\n",
        "    {'source': 'AP - Economy',  'url': 'https://rsshub.app/apnews/topics/economy'},\n",
        "    {'source': 'AP - Finance',  'url': 'https://rsshub.app/apnews/topics/financial-markets'},\n",
        "    # Forbes\n",
        "    {'source': 'Forbes - Business',  'url': 'https://www.forbes.com/business/feed/'},\n",
        "    {'source': 'Forbes - Markets',   'url': 'https://www.forbes.com/money/feed/'},\n",
        "    {'source': 'Forbes - Investing', 'url': 'https://www.forbes.com/investing/feed/'},\n",
        "    # MarketWatch\n",
        "    {'source': 'MarketWatch - Top Stories',  'url': 'https://feeds.content.dowjones.io/public/rss/mw_topstories'},\n",
        "    {'source': 'MarketWatch - Market Pulse', 'url': 'https://feeds.content.dowjones.io/public/rss/mw_marketpulse'},\n",
        "    {'source': 'MarketWatch - Real Estate',  'url': 'https://feeds.content.dowjones.io/public/rss/mw_realestate'},\n",
        "    # Seeking Alpha\n",
        "    {'source': 'Seeking Alpha - Market News', 'url': 'https://seekingalpha.com/market_currents.xml'},\n",
        "    {'source': 'Seeking Alpha - Wall Street', 'url': 'https://seekingalpha.com/feed/wall-street-breakfast'},\n",
        "    # Barron's\n",
        "    {'source': \"Barron's\", 'url': 'https://www.barrons.com/xml/rss/3_7510.xml'},\n",
        "    # The Economist\n",
        "    {'source': 'The Economist - Finance', 'url': 'https://www.economist.com/finance-and-economics/rss.xml'},\n",
        "    {'source': 'The Economist - Business','url': 'https://www.economist.com/business/rss.xml'},\n",
        "    # Nasdaq\n",
        "    {'source': 'Nasdaq - Markets',  'url': 'https://www.nasdaq.com/feed/rssoutbound?category=Markets'},\n",
        "    {'source': 'Nasdaq - Original', 'url': 'https://www.nasdaq.com/feed/rssoutbound?category=Original+Articles'},\n",
        "]\n",
        "\n",
        "\n",
        "class RSSFinancialScraper:\n",
        "    \"\"\"‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Multi-Source RSS ‡∏î‡πâ‡∏ß‡∏¢ feedparser\"\"\"\n",
        "\n",
        "    def __init__(self, target: int = 700, fetch_full_article: bool = True,\n",
        "                 delay: tuple = (0.5, 1.5)):\n",
        "        self.target      = target\n",
        "        self.fetch_full  = fetch_full_article\n",
        "        self.delay       = delay\n",
        "        self.articles    = []\n",
        "        self.seen_titles = set()\n",
        "\n",
        "    def _sleep(self):\n",
        "        time.sleep(random.uniform(*self.delay))\n",
        "\n",
        "    def _extract_full_content(self, url: str) -> str:\n",
        "        \"\"\"‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å article URL\"\"\"\n",
        "        try:\n",
        "            self._sleep()\n",
        "            r    = requests.get(url, headers=HEADERS, timeout=12)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, 'html.parser')\n",
        "            for sel in [\n",
        "                {'tag': 'article'},\n",
        "                {'tag': 'div', 'class_': re.compile(r'article.body|story.body|content.body|post.body', re.I)},\n",
        "                {'tag': 'div', 'id':     re.compile(r'article|content|main', re.I)},\n",
        "            ]:\n",
        "                tag_name = sel.pop('tag')\n",
        "                el       = soup.find(tag_name, **sel)\n",
        "                if el:\n",
        "                    text = ' '.join(p.get_text(' ', strip=True) for p in el.find_all('p'))\n",
        "                    if len(text.split()) > 30:\n",
        "                        return text\n",
        "            return ' '.join(p.get_text(' ', strip=True) for p in soup.find_all('p'))\n",
        "        except Exception as e:\n",
        "            logger.debug(f'[HTML ERROR] {url[:60]} ‚Üí {e}')\n",
        "            return ''\n",
        "\n",
        "    def _parse_rss(self, feed_url: str, source: str) -> int:\n",
        "        \"\"\"Parse RSS feed ‡∏î‡πâ‡∏ß‡∏¢ feedparser\"\"\"\n",
        "        added = 0\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url, agent=HEADERS['User-Agent'])\n",
        "            if feed.bozo and not feed.entries:\n",
        "                return 0\n",
        "            for entry in feed.entries:\n",
        "                if len(self.articles) >= self.target:\n",
        "                    break\n",
        "                title = getattr(entry, 'title', '').strip()\n",
        "                if not title or title in self.seen_titles:\n",
        "                    continue\n",
        "                self.seen_titles.add(title)\n",
        "                url = getattr(entry, 'link', '')\n",
        "                # Content: content > summary > description\n",
        "                content = ''\n",
        "                if hasattr(entry, 'content') and entry.content:\n",
        "                    content = entry.content[0].get('value', '')\n",
        "                if not content and hasattr(entry, 'summary'):\n",
        "                    content = entry.summary\n",
        "                if not content and hasattr(entry, 'description'):\n",
        "                    content = entry.description\n",
        "                content = BeautifulSoup(content, 'html.parser').get_text(' ', strip=True)\n",
        "                # fetch full article ‡∏ñ‡πâ‡∏≤ content ‡∏™‡∏±‡πâ‡∏ô\n",
        "                if self.fetch_full and url and len(content.split()) < 80:\n",
        "                    full = self._extract_full_content(url)\n",
        "                    if full:\n",
        "                        content = full\n",
        "                if not content or len(content.split()) < 10:\n",
        "                    continue\n",
        "                pub = getattr(entry, 'published', getattr(entry, 'updated', ''))\n",
        "                try:\n",
        "                    from email.utils import parsedate_to_datetime\n",
        "                    dt         = parsedate_to_datetime(pub)\n",
        "                    clean_date = dt.strftime('%Y-%m-%d')\n",
        "                    year       = dt.year\n",
        "                except:\n",
        "                    clean_date, year = TODAY, 2025\n",
        "                self.articles.append({\n",
        "                    'title':    title,\n",
        "                    'content':  content[:1000],\n",
        "                    'date':     clean_date,\n",
        "                    'year':     year,\n",
        "                    'source':   source,\n",
        "                    'category': source.split(' - ')[-1].lower().replace(' ', '_'),\n",
        "                })\n",
        "                added += 1\n",
        "        except Exception as e:\n",
        "            logger.warning(f'[RSS ERROR] {source}: {e}')\n",
        "        return added\n",
        "\n",
        "    def collect(self) -> pd.DataFrame:\n",
        "        logger.info(f'üöÄ Multi-source RSS ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ {self.target} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏à‡∏≤‡∏Å {len(MULTI_RSS_FEEDS)} feeds')\n",
        "        for i, feed in enumerate(MULTI_RSS_FEEDS):\n",
        "            if len(self.articles) >= self.target:\n",
        "                break\n",
        "            n = self._parse_rss(feed['url'], feed['source'])\n",
        "            logger.info(f'  [{i+1:>2}/{len(MULTI_RSS_FEEDS)}] {feed[\"source\"]:<35} +{n} | ‡∏£‡∏ß‡∏° {len(self.articles)}/{self.target}')\n",
        "        logger.info(f'‚úÖ ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ {len(self.articles)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "        return pd.DataFrame(self.articles)\n",
        "\n",
        "\n",
        "scraper  = RSSFinancialScraper(target=700, fetch_full_article=True, delay=(0.5, 1.5))\n",
        "df_multi = scraper.collect()\n",
        "print(f'\\nüìä Multi-Source RSS ‡∏£‡∏ß‡∏°: {len(df_multi)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')"
      ],
      "id": "-30dM2j2tSGw"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "8CR99iSJtSGy",
        "outputId": "b193706a-e44b-4ff6-a4bc-8e12ac50d253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ‡∏£‡∏ß‡∏° Dataset ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : 956 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  : 14 ‡πÅ‡∏´‡∏•‡πà‡∏á\n",
            "   ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: [2025, 2026]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title content        date  \\\n",
              "0        Hackett Group HCKT Earnings Call Transcript          2026-02-19   \n",
              "1  Labcorp Holdings Inc. Q4 2025 Earnings Call Su...          2026-02-19   \n",
              "2  ReNew Energy Global Plc Q3 2026 Earnings Call ...          2026-02-19   \n",
              "\n",
              "   year         source     category  \n",
              "0  2025  Yahoo Finance  top_stories  \n",
              "1  2025  Yahoo Finance  top_stories  \n",
              "2  2025  Yahoo Finance  top_stories  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a26df881-9bbd-492b-83c7-8ac8af324990\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>source</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hackett Group HCKT Earnings Call Transcript</td>\n",
              "      <td></td>\n",
              "      <td>2026-02-19</td>\n",
              "      <td>2025</td>\n",
              "      <td>Yahoo Finance</td>\n",
              "      <td>top_stories</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Labcorp Holdings Inc. Q4 2025 Earnings Call Su...</td>\n",
              "      <td></td>\n",
              "      <td>2026-02-19</td>\n",
              "      <td>2025</td>\n",
              "      <td>Yahoo Finance</td>\n",
              "      <td>top_stories</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ReNew Energy Global Plc Q3 2026 Earnings Call ...</td>\n",
              "      <td></td>\n",
              "      <td>2026-02-19</td>\n",
              "      <td>2025</td>\n",
              "      <td>Yahoo Finance</td>\n",
              "      <td>top_stories</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a26df881-9bbd-492b-83c7-8ac8af324990')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a26df881-9bbd-492b-83c7-8ac8af324990 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a26df881-9bbd-492b-83c7-8ac8af324990');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_raw",
              "summary": "{\n  \"name\": \"df_raw\",\n  \"rows\": 956,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 956,\n        \"samples\": [\n          \"Nvidia Sold Its Stakes in These Firms. The Stocks Are Sliding.\",\n          \"Armin Papperger\\u2019s vaulting ambitions for Rheinmetall\",\n          \"FDA chief warns U.S. is losing ground to China in early drug development, calls for faster trial approvals\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 907,\n        \"samples\": [\n          \"Companies run on decisions. Asking three questions makes choices better\",\n          \"Have money, will travel: a16z\\u2019s hunt for the next European unicorn\",\n          \"Dollar Rises as Currency Traders Bet on Fewer Fed Rate Cuts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 171,\n        \"samples\": [\n          \"2025-07-08\",\n          \"2025-10-30\",\n          \"2025-10-29\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2025,\n        \"max\": 2026,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2026,\n          2025\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Seeking Alpha - Market News\",\n          \"The Economist - Business\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"business\",\n          \"ethereum\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  1.1.4  ‡∏£‡∏ß‡∏° Dataset ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î + ‡∏Å‡∏£‡∏≠‡∏á 2025‚Äì2026\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "all_dfs = [df for df in [df_yahoo_rss, df_yahoo_html, df_multi] if not df.empty]\n",
        "\n",
        "if all_dfs:\n",
        "    df_raw = pd.concat(all_dfs, ignore_index=True)\n",
        "    df_raw = df_raw.drop_duplicates(subset=['title']).reset_index(drop=True)\n",
        "    df_raw = df_raw[df_raw['year'].isin([2025, 2026])].reset_index(drop=True)\n",
        "else:\n",
        "    # ‚îÄ‚îÄ‚îÄ Fallback Demo Dataset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print('‚ö†Ô∏è  Scraping ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏• ‚Äî ‡πÉ‡∏ä‡πâ Demo Dataset ‡πÅ‡∏ó‡∏ô')\n",
        "    df_raw = pd.DataFrame([\n",
        "        {'title': 'Apple reports record Q1 2025 revenue beating all forecasts',\n",
        "         'content': 'Apple Inc. surpassed Wall Street expectations with record-breaking Q1 2025 revenue driven by strong iPhone and services growth.',\n",
        "         'date': '2025-02-10', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'apple'},\n",
        "        {'title': 'Federal Reserve keeps interest rates unchanged in February 2025',\n",
        "         'content': 'The Federal Reserve held its benchmark rate steady, signaling caution amid mixed economic data and persistent inflation concerns.',\n",
        "         'date': '2025-02-01', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'top_stories'},\n",
        "        {'title': 'Nvidia stock surges 8 percent after strong earnings guidance',\n",
        "         'content': 'Nvidia Corporation shares jumped sharply following better-than-expected earnings and bullish guidance tied to AI chip demand.',\n",
        "         'date': '2025-02-12', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'nvidia'},\n",
        "        {'title': 'Bitcoin falls below 90000 as crypto market sees correction',\n",
        "         'content': 'Cryptocurrency markets experienced a sharp decline with Bitcoin dropping below the 90,000 dollar mark amid profit-taking.',\n",
        "         'date': '2025-02-05', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'bitcoin'},\n",
        "        {'title': 'Tesla shares drop 12 percent on weak delivery numbers',\n",
        "         'content': 'Tesla Inc. reported lower-than-expected deliveries for Q1 2025, disappointing investors and triggering a sharp stock selloff.',\n",
        "         'date': '2025-02-03', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'tesla'},\n",
        "        {'title': 'S&P 500 hits new record as tech stocks lead market rally',\n",
        "         'content': 'The benchmark S&P 500 index closed at a new record high, powered by broad gains in mega-cap technology stocks.',\n",
        "         'date': '2025-02-14', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'sp500'},\n",
        "        {'title': 'JPMorgan raises dividend after strong fourth quarter earnings',\n",
        "         'content': 'JPMorgan Chase announced an increase to its quarterly dividend following a robust Q4 2024 earnings report that beat analyst estimates.',\n",
        "         'date': '2025-01-20', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'jpmorgan'},\n",
        "        {'title': 'Oil prices rise on OPEC production cut agreement for 2025',\n",
        "         'content': 'Crude oil futures climbed higher after OPEC+ members agreed to extend production cuts through mid-2025 to support prices.',\n",
        "         'date': '2025-01-15', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'oil'},\n",
        "        {'title': 'Gold hits all-time high as investors seek safe haven assets',\n",
        "         'content': 'Gold futures surged to a record high amid heightened geopolitical tensions and growing demand for safe-haven assets globally.',\n",
        "         'date': '2025-02-08', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'gold'},\n",
        "        {'title': 'US inflation drops to 2.5 percent lowest level since 2021',\n",
        "         'content': 'Consumer prices rose at a slower pace in January 2025, bringing inflation to its lowest point since 2021 and bolstering rate-cut hopes.',\n",
        "         'date': '2025-02-13', 'year': 2025, 'source': 'Yahoo Finance', 'category': 'economy'},\n",
        "    ])\n",
        "\n",
        "print(f'‚úÖ ‡∏£‡∏ß‡∏° Dataset ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')\n",
        "print(f'   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : {len(df_raw):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "print(f'   ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  : {df_raw[\"source\"].nunique()} ‡πÅ‡∏´‡∏•‡πà‡∏á')\n",
        "print(f'   ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {sorted(df_raw[\"year\"].unique().tolist())}')\n",
        "df_raw.head(3)"
      ],
      "id": "8CR99iSJtSGy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152TrUF2tSGz"
      },
      "source": [
        "## Section 1.2 ‚Äî Cleaning Pipeline ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å `clean_text_(Web_Scraping).csv`"
      ],
      "id": "152TrUF2tSGz"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TozaCs_6tSGz",
        "outputId": "9f897f92-7f20-4d7d-a371-c47d3abb8f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaning Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
            "   ‡∏Å‡πà‡∏≠‡∏ô clean : 956 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   ‡∏´‡∏•‡∏±‡∏á clean : 839 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   ‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å     : 117 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏ß‡πà‡∏≤‡∏á / ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô)\n",
            "\n",
            "üìä Web Scraping Dataset\n",
            "   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Sentence : 839 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‚Üí output_data/clean_text_(Web_Scraping).csv\n",
            "\n",
            "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 Sentence ‡πÅ‡∏£‡∏Å ---\n",
            "[1] Three Top Dividend Stocks To Consider. As February begins, U.S. stock indexes have shown strength, with the Dow Jones Industrial Average adding 515 points and the S&P 500 nearing a\n",
            "\n",
            "[2] Dow Jones Futures Fall After S&P 500 Hits Resistance; Carvana Dives, Walmart Due. Futures fall after the stock market extended a bounce Wednesday, but the S&P 500 hit resistance. C\n",
            "\n",
            "[3] Here's Why I Wouldn't Touch Medical Properties Trust With a 10 Foot Pole. Medical Properties Trust has a huge 6.6% yield, but the dividend history is a big problem for me.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  1.2  CLEANING PIPELINE\n",
        "#  HTML tags ‚Üí URLs ‚Üí Emojis ‚Üí Special chars ‚Üí Whitespace\n",
        "#  ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô build Sentence (title ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô + content)\n",
        "#  ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å clean_text_(Web_Scraping).csv\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "EMOJI_RE = re.compile(\n",
        "    '['\n",
        "    u'\\U0001F600-\\U0001F64F'\n",
        "    u'\\U0001F300-\\U0001F5FF'\n",
        "    u'\\U0001F680-\\U0001F6FF'\n",
        "    u'\\U0001F1E0-\\U0001F1FF'\n",
        "    u'\\U00002702-\\U000027B0'\n",
        "    u'\\U000024C2-\\U0001F251'\n",
        "    ']+', flags=re.UNICODE\n",
        ")\n",
        "\n",
        "\n",
        "def clean_field(text: str) -> str:\n",
        "    \"\"\"\n",
        "    ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î 1 field:\n",
        "    1. ‡∏•‡∏ö HTML tags   (BeautifulSoup)\n",
        "    2. ‡∏•‡∏ö URLs        (http/https/www)\n",
        "    3. ‡∏•‡∏ö Emojis      (Unicode ranges)\n",
        "    4. ‡∏•‡∏ö special chars (‡πÄ‡∏Å‡πá‡∏ö a-z A-Z 0-9 .,!?;:'\\\"%-+&)\n",
        "    5. Normalize whitespace\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return ''\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text(separator=' ')   # 1\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)               # 2\n",
        "    text = EMOJI_RE.sub(' ', text)                                       # 3\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?;:'\\\"&%+-]\", ' ', text)         # 4\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()                            # 5\n",
        "    return text\n",
        "\n",
        "\n",
        "def build_sentence(title: str, content: str) -> str:\n",
        "    \"\"\"\n",
        "    ‡∏£‡∏ß‡∏° title + content ‡πÄ‡∏õ‡πá‡∏ô Sentence ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "    - title ‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡πÄ‡∏™‡∏°‡∏≠  (‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ó‡∏£‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏∑‡πà‡∏ô)\n",
        "    - ‡∏ñ‡πâ‡∏≤ content ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á: '<title>. <content>'\n",
        "    - ‡∏ñ‡πâ‡∏≤ content ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö title (‡∏ö‡∏≤‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á copy title ‡πÄ‡∏õ‡πá‡∏ô content): ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà title\n",
        "    - normalize whitespace ‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
        "    \"\"\"\n",
        "    title   = str(title).strip()\n",
        "    content = str(content).strip()\n",
        "\n",
        "    if title and content:\n",
        "        if content.lower().startswith(title.lower()):\n",
        "            sentence = content\n",
        "        else:\n",
        "            sep      = '' if title.endswith('.') else '. '\n",
        "            sentence = title + sep + content\n",
        "    elif title:\n",
        "        sentence = title\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "    # Final normalize\n",
        "    sentence = re.sub(r'[ \\t]+', ' ', sentence)\n",
        "    sentence = re.sub(r'\\n+',    ' ', sentence)\n",
        "    return sentence.strip()\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Apply cleaning ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "df_clean = df_raw.copy()\n",
        "df_clean['title_clean']   = df_clean['title'].fillna('').apply(clean_field)\n",
        "df_clean['content_clean'] = df_clean['content'].fillna('').apply(clean_field)\n",
        "df_clean['word_count']    = df_clean['content_clean'].str.split().str.len()\n",
        "\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà title ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠ content < 10 ‡∏Ñ‡∏≥\n",
        "before   = len(df_clean)\n",
        "df_clean = df_clean[\n",
        "    (df_clean['title_clean'].str.len() > 0) &\n",
        "    (df_clean['content_clean'].str.len() > 0) &\n",
        "    (df_clean['word_count'] >= 10)\n",
        "].reset_index(drop=True)\n",
        "\n",
        "print('‚úÖ Cleaning Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô')\n",
        "print(f'   ‡∏Å‡πà‡∏≠‡∏ô clean : {before:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "print(f'   ‡∏´‡∏•‡∏±‡∏á clean : {len(df_clean):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "print(f'   ‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å     : {before - len(df_clean):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏ß‡πà‡∏≤‡∏á / ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô)')\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Build Sentence ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "df_clean['Sentence'] = df_clean.apply(\n",
        "    lambda r: build_sentence(r['title_clean'], r['content_clean']), axis=1\n",
        ")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ ‡∏™‡∏£‡πâ‡∏≤‡∏á Web Scraping CSV (Sentence column ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "df_web = (\n",
        "    df_clean[['Sentence']]\n",
        "    .copy()\n",
        "    .assign(Sentence=lambda d: (\n",
        "        d['Sentence']\n",
        "        .str.replace(r'[ \\t]+', ' ', regex=True)\n",
        "        .str.replace(r'\\n+',    ' ', regex=True)\n",
        "        .str.strip()\n",
        "    ))\n",
        ")\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà Sentence ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô\n",
        "df_web = df_web[df_web['Sentence'].str.split().str.len() >= 5].reset_index(drop=True)\n",
        "\n",
        "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\n",
        "df_web.to_csv(WEB_CSV, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f'\\nüìä Web Scraping Dataset')\n",
        "print(f'   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Sentence : {len(df_web):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "print(f'üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‚Üí {WEB_CSV}')\n",
        "print()\n",
        "print('--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 Sentence ‡πÅ‡∏£‡∏Å ---')\n",
        "for i, row in df_web.head(3).iterrows():\n",
        "    print(f'[{i+1}] {row[\"Sentence\"][:180]}')\n",
        "    print()"
      ],
      "id": "TozaCs_6tSGz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZPbephTtSG0"
      },
      "source": [
        "## Section 1.2.5 ‚Äî Merge Kaggle Dataset ‚Üí `complete_dataset.csv`\n",
        "\n",
        "> **‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ:** ‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° **\"üìÇ Browse Kaggle CSV\"** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå dataset ‡∏à‡∏≤‡∏Å Kaggle ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì  \n",
        "> ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏∞‡∏ï‡∏±‡∏î column ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≠‡∏Å‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞ `Sentence` ‡πÅ‡∏•‡πâ‡∏ß merge ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö Web Scraping dataset"
      ],
      "id": "-ZPbephTtSG0"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "hhKO8yAStSG0",
        "outputId": "f7641bf3-50b1-4dee-ec5d-a9894bce16b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Kaggle dataset (.csv) ‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-74a73ff8-f4b5-41af-8b94-5a3b4d25b767\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-74a73ff8-f4b5-41af-8b94-5a3b4d25b767\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Financial Sentiment Analysis 1.csv to Financial Sentiment Analysis 1.csv\n",
            "\n",
            "üìÑ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: Financial Sentiment Analysis 1.csv  (727.7 KB)\n",
            "üìã Columns ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : ['Sentence', 'Sentiment']\n",
            "   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß       : 5,842\n",
            "\n",
            "üîç ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ column : \"Sentence\" ‡πÄ‡∏õ‡πá‡∏ô Sentence\n",
            "   ‡∏´‡∏•‡∏±‡∏á clean     : 5,781 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "============================================================\n",
            "‚úÖ Merge Dataset ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
            "   Web Scraping : 839 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   Kaggle       : 5,781 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   ‡∏£‡∏ß‡∏° (dedup) : 6,095 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‚Üí output_data/complete_dataset.csv\n",
            "\n",
            "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 Sentence ‡πÅ‡∏£‡∏Å ---\n",
            "[1] Three Top Dividend Stocks To Consider. As February begins, U.S. stock indexes have shown strength, with the Dow Jones Industrial Average adding 515 points and the S&P 500 nearing a\n",
            "\n",
            "[2] Dow Jones Futures Fall After S&P 500 Hits Resistance; Carvana Dives, Walmart Due. Futures fall after the stock market extended a bounce Wednesday, but the S&P 500 hit resistance. C\n",
            "\n",
            "[3] Here's Why I Wouldn't Touch Medical Properties Trust With a 10 Foot Pole. Medical Properties Trust has a huge 6.6% yield, but the dividend history is a big problem for me.\n",
            "\n",
            "[4] Tech Qualms, Walmart Outlook Damp Wall Street Pre-Bell; Asia Up, Europe Off. Wall Street futures pointed moderately lower pre-bell Thursday as tech mega-caps gave up some gains\n",
            "\n",
            "[5] Does Index Upgrades for Ciena CIEN Change The Bull Case For Its AI-Optical Story?. Ciena Corporation s recent addition to the S&P 500 Equal Weighted Index and the S&P Global 1200 h\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  1.2.5  MERGE KAGGLE DATASET (Google Colab version)\n",
        "#  Upload file ‚Üí ‡∏ï‡∏±‡∏î column ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà Sentence ‚Üí merge\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "import io\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Helper: ‡∏´‡∏≤ column ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Sentence ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def _find_sentence_col(df):\n",
        "    \"\"\"\n",
        "    ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ column ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß:\n",
        "    1. ‡∏ä‡∏∑‡πà‡∏≠ 'Sentence' ‡∏ï‡∏£‡∏á‡πÜ (case-insensitive)\n",
        "    2. ‡∏ä‡∏∑‡πà‡∏≠ 'text', 'content', 'article', 'body', 'description', 'news'\n",
        "    3. column string ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (fallback)\n",
        "    \"\"\"\n",
        "    col_lower = {c.lower(): c for c in df.columns}\n",
        "    for keyword in ['sentence', 'text', 'content', 'article', 'body', 'description', 'news']:\n",
        "        if keyword in col_lower:\n",
        "            return col_lower[keyword]\n",
        "    # fallback: column string ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢\n",
        "    str_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "    if str_cols:\n",
        "        avg_len = {c: df[c].astype(str).str.len().mean() for c in str_cols}\n",
        "        return max(avg_len, key=avg_len.get)\n",
        "    return None\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Step 1: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print('üìÇ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Kaggle dataset (.csv) ‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì')\n",
        "uploaded = files.upload()   # ‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î file picker ‡∏Ç‡∏≠‡∏á Colab ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
        "\n",
        "if not uploaded:\n",
        "    print('‚ùå ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô')\n",
        "else:\n",
        "    # ‚îÄ‚îÄ‚îÄ Step 2: ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    fname    = list(uploaded.keys())[0]\n",
        "    raw_bytes = uploaded[fname]\n",
        "\n",
        "    print(f'\\nüìÑ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: {fname}  ({len(raw_bytes)/1024:.1f} KB)')\n",
        "\n",
        "    try:\n",
        "        df_kaggle_raw = pd.read_csv(io.BytesIO(raw_bytes), encoding='utf-8-sig')\n",
        "    except UnicodeDecodeError:\n",
        "        df_kaggle_raw = pd.read_csv(io.BytesIO(raw_bytes), encoding='latin-1')\n",
        "\n",
        "    print(f'üìã Columns ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : {list(df_kaggle_raw.columns)}')\n",
        "    print(f'   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß       : {len(df_kaggle_raw):,}')\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ Step 3: ‡∏´‡∏≤ column ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    src_col = _find_sentence_col(df_kaggle_raw)\n",
        "    if src_col is None:\n",
        "        print('‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö column ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå')\n",
        "    else:\n",
        "        print(f'\\nüîç ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ column : \"{src_col}\" ‡πÄ‡∏õ‡πá‡∏ô Sentence')\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ Step 4: ‡∏ï‡∏±‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà Sentence + clean ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        df_kaggle = pd.DataFrame({'Sentence': df_kaggle_raw[src_col].astype(str)})\n",
        "        df_kaggle['Sentence'] = (\n",
        "            df_kaggle['Sentence']\n",
        "            .apply(clean_field)                         # clean_field ‡∏à‡∏≤‡∏Å Section 1.2\n",
        "            .str.replace(r'[ \\t]+', ' ', regex=True)\n",
        "            .str.replace(r'\\n+',    ' ', regex=True)\n",
        "            .str.strip()\n",
        "        )\n",
        "        # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠ < 5 ‡∏Ñ‡∏≥\n",
        "        df_kaggle = df_kaggle[\n",
        "            df_kaggle['Sentence'].str.strip().str.len() > 0\n",
        "        ].reset_index(drop=True)\n",
        "        df_kaggle = df_kaggle[\n",
        "            df_kaggle['Sentence'].str.split().str.len() >= 5\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "        print(f'   ‡∏´‡∏•‡∏±‡∏á clean     : {len(df_kaggle):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ Step 5: Merge ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö Web Scraping ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        df_complete = pd.concat([df_web, df_kaggle], ignore_index=True)\n",
        "        df_complete = df_complete.drop_duplicates(subset=['Sentence']).reset_index(drop=True)\n",
        "        df_complete['Sentence'] = (\n",
        "            df_complete['Sentence']\n",
        "            .str.replace(r'[ \\t]+', ' ', regex=True)\n",
        "            .str.replace(r'\\n+',    ' ', regex=True)\n",
        "            .str.strip()\n",
        "        )\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ Step 6: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å complete_dataset.csv ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        df_complete.to_csv(COMPLETE_CSV, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print()\n",
        "        print('=' * 60)\n",
        "        print('‚úÖ Merge Dataset ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!')\n",
        "        print(f'   Web Scraping : {len(df_web):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "        print(f'   Kaggle       : {len(df_kaggle):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "        print(f'   ‡∏£‡∏ß‡∏° (dedup) : {len(df_complete):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "        print(f'üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‚Üí {COMPLETE_CSV}')\n",
        "        print()\n",
        "        print('--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 Sentence ‡πÅ‡∏£‡∏Å ---')\n",
        "        for i, row in df_complete.head(5).iterrows():\n",
        "            print(f'[{i+1}] {row[\"Sentence\"][:180]}')\n",
        "            print()"
      ],
      "id": "hhKO8yAStSG0"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "4Pa5KHEWtSG1",
        "outputId": "27ce4d9a-c7bf-43db-c0d2-a542435f5241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î complete_dataset.csv ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Sentence : 6,095 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "   Column         : ['Sentence']\n",
            "\n",
            "üìÅ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô output_data/:\n",
            "   ‚úÖ clean_text_(Web_Scraping).csv  (322.1 KB)\n",
            "   ‚úÖ complete_dataset.csv  (923.7 KB)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Sentence\n",
              "0  Three Top Dividend Stocks To Consider. As Febr...\n",
              "1  Dow Jones Futures Fall After S&P 500 Hits Resi...\n",
              "2  Here's Why I Wouldn't Touch Medical Properties..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27673f29-0a87-4ee8-9098-002385fbd4f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Three Top Dividend Stocks To Consider. As Febr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dow Jones Futures Fall After S&amp;P 500 Hits Resi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Here's Why I Wouldn't Touch Medical Properties...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27673f29-0a87-4ee8-9098-002385fbd4f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-27673f29-0a87-4ee8-9098-002385fbd4f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-27673f29-0a87-4ee8-9098-002385fbd4f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_complete",
              "summary": "{\n  \"name\": \"df_complete\",\n  \"rows\": 6095,\n  \"fields\": [\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6095,\n        \"samples\": [\n          \"Validating our fgVoIP client through Symbian Signed represents a significant step forward in accomplishing this goal .\",\n          \"We are now taking Marimekko there on a distinctly more significant scale .\",\n          \"Margin call of Zanadvorov has given the chance to make such purchase under the credit of Deutsche Bank for USD 560 million .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  ‡πÇ‡∏´‡∏•‡∏î complete_dataset.csv ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Section 1.3\n",
        "#  (‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Merge ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "if os.path.exists(COMPLETE_CSV):\n",
        "    df_complete = pd.read_csv(COMPLETE_CSV, encoding='utf-8-sig')\n",
        "    print(f'‚úÖ ‡πÇ‡∏´‡∏•‡∏î complete_dataset.csv ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')\n",
        "    print(f'   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Sentence : {len(df_complete):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "    print(f'   Column         : {list(df_complete.columns)}')\n",
        "else:\n",
        "    # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ merge ‚Üí ‡πÉ‡∏ä‡πâ Web Scraping ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡πà‡∏≠‡∏ô (fallback)\n",
        "    print('‚ö†Ô∏è  ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö complete_dataset.csv')\n",
        "    print('    ‚Üí ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô Section 1.2.5 (Merge Kaggle) ‡∏Å‡πà‡∏≠‡∏ô')\n",
        "    print('    ‚Üí ‡πÉ‡∏ä‡πâ Web Scraping dataset ‡πÅ‡∏ó‡∏ô‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß')\n",
        "    df_complete = df_web.copy()\n",
        "\n",
        "# ‚îÄ‚îÄ ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å output_data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "_unwanted = ['raw_news.csv', 'processed_news.csv']\n",
        "for fname in _unwanted:\n",
        "    fpath = f'{OUT_DIR}/{fname}'\n",
        "    if os.path.exists(fpath):\n",
        "        os.remove(fpath)\n",
        "        print(f'üóëÔ∏è  ‡∏•‡∏ö {fname} ‡πÅ‡∏•‡πâ‡∏ß')\n",
        "\n",
        "# ‚îÄ‚îÄ ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô output_data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(f'\\nüìÅ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô {OUT_DIR}/:')\n",
        "for f in sorted(os.listdir(OUT_DIR)):\n",
        "    if f.endswith('.csv') and not f.startswith('_'):\n",
        "        size = os.path.getsize(f'{OUT_DIR}/{f}') / 1024\n",
        "        print(f'   ‚úÖ {f}  ({size:.1f} KB)')\n",
        "\n",
        "df_complete.head(3)"
      ],
      "id": "4Pa5KHEWtSG1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mcLTawgtSG1"
      },
      "source": [
        "## Section 1.3 ‚Äî Tokenization Comparison (NLTK vs spaCy)\n",
        "\n",
        "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ 2 ‡∏ß‡∏¥‡∏ò‡∏µ ‡∏ö‡∏ô `complete_dataset.csv`:\n",
        "- **NLTK** `word_tokenize` + stopword removal (word-level, regex-based, ‡πÄ‡∏£‡πá‡∏ß)\n",
        "- **spaCy** lemmatize + stopword removal (statistical model, ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤)"
      ],
      "id": "9mcLTawgtSG1"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pllVJiOBtSG1",
        "outputId": "142fb4ca-1150-4995-f720-ae6957dc6086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Tokenization Comparison: NLTK vs spaCy\n",
            "=================================================================\n",
            "\n",
            "[1] Three Top Dividend Stocks To Consider. As February begins, U.S. stock \n",
            "    NLTK  ( 39 tokens,   0.6ms): ['three', 'top', 'dividend', 'stocks', 'consider', 'february', 'begins', 'stock']\n",
            "    spaCy ( 36 tokens,  17.1ms): ['dividend', 'stock', 'consider', 'february', 'begin', 'stock', 'index', 'show']\n",
            "\n",
            "[2] Dow Jones Futures Fall After S&P 500 Hits Resistance; Carvana Dives, W\n",
            "    NLTK  ( 27 tokens,   0.4ms): ['dow', 'jones', 'futures', 'fall', 'p', 'hits', 'resistance', 'carvana']\n",
            "    spaCy ( 24 tokens,  11.8ms): ['dow', 'jones', 'futures', 'fall', 'hits', 'resistance', 'carvana', 'dives']\n",
            "\n",
            "[3] Here's Why I Wouldn't Touch Medical Properties Trust With a 10 Foot Po\n",
            "    NLTK  ( 16 tokens,   0.3ms): ['would', 'touch', 'medical', 'properties', 'trust', 'foot', 'pole', 'medical']\n",
            "    spaCy ( 15 tokens,  10.8ms): ['touch', 'medical', 'properties', 'trust', 'foot', 'pole', 'medical', 'properties']\n",
            "\n",
            "[4] Tech Qualms, Walmart Outlook Damp Wall Street Pre-Bell; Asia Up, Europ\n",
            "    NLTK  ( 19 tokens,   0.3ms): ['tech', 'qualms', 'walmart', 'outlook', 'damp', 'wall', 'street', 'asia']\n",
            "    spaCy ( 25 tokens,  10.5ms): ['tech', 'qualms', 'walmart', 'outlook', 'damp', 'wall', 'street', 'pre']\n",
            "\n",
            "[5] Does Index Upgrades for Ciena CIEN Change The Bull Case For Its AI-Opt\n",
            "    NLTK  ( 59 tokens,   0.6ms): ['index', 'upgrades', 'ciena', 'cien', 'change', 'bull', 'case', 'story']\n",
            "    spaCy ( 58 tokens,  20.0ms): ['index', 'upgrades', 'ciena', 'cien', 'change', 'bull', 'case', 'ai']\n",
            "\n",
            "=================================================================\n",
            "üìä ‡∏™‡∏£‡∏∏‡∏õ Tokenization Comparison (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 5 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
            "=================================================================\n",
            "  NLTK  ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢   32.0 tokens  |    0.5 ms\n",
            "  spaCy ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢   31.6 tokens  |   14.1 ms\n",
            "\n",
            "üìå ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n",
            "  ‚Ä¢ NLTK  ‚Üí regex-based tokenizer, ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà\n",
            "  ‚Ä¢ spaCy ‚Üí statistical model, ‡πÉ‡∏ä‡πâ lemmatization (running‚Üírun)\n",
            "           ‡∏ó‡∏≥‡πÉ‡∏´‡πâ vocab ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å POS/NER ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß\n",
            "  ‚Ä¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å: spaCy ‡πÅ‡∏¢‡∏Å punctuation ‡∏ä‡∏±‡∏î‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏•‡∏∞\n",
            "    lemmatize ‡∏ó‡∏≥‡πÉ‡∏´‡πâ token count ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ NLTK ‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  1.3  TOKENIZATION COMPARISON ‚Äî NLTK vs spaCy\n",
        "#  ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ö‡∏ô complete_dataset.csv\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import time as _time\n",
        "\n",
        "STOPWORDS_EN = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def tok_nltk(text: str) -> list:\n",
        "    \"\"\"Word-level: NLTK word_tokenize + stopword removal\"\"\"\n",
        "    tokens = word_tokenize(str(text).lower())\n",
        "    return [t for t in tokens if t.isalpha() and t not in STOPWORDS_EN]\n",
        "\n",
        "\n",
        "def tok_spacy(text: str) -> list:\n",
        "    \"\"\"Word-level: spaCy lemmatize + stopword removal\"\"\"\n",
        "    doc = nlp(str(text)[:50_000])\n",
        "    return [tok.lemma_.lower() for tok in doc\n",
        "            if tok.is_alpha and not tok.is_stop and len(tok.text) > 1]\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ö‡∏ô 5 ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏£‡∏Å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "sample_texts = df_complete['Sentence'].head(5).tolist()\n",
        "print('üî§ Tokenization Comparison: NLTK vs spaCy')\n",
        "print('=' * 65)\n",
        "\n",
        "comparison_rows = []\n",
        "for i, text in enumerate(sample_texts):\n",
        "    t0      = _time.time()\n",
        "    n_toks  = tok_nltk(text)\n",
        "    nltk_ms = (_time.time() - t0) * 1000\n",
        "\n",
        "    t0      = _time.time()\n",
        "    s_toks  = tok_spacy(text)\n",
        "    spacy_ms= (_time.time() - t0) * 1000\n",
        "\n",
        "    comparison_rows.append({\n",
        "        'text_preview':  text[:60] + ('...' if len(text) > 60 else ''),\n",
        "        'nltk_count':    len(n_toks),\n",
        "        'spacy_count':   len(s_toks),\n",
        "        'nltk_time_ms':  round(nltk_ms,  2),\n",
        "        'spacy_time_ms': round(spacy_ms, 2),\n",
        "        'nltk_sample':   n_toks[:8],\n",
        "        'spacy_sample':  s_toks[:8],\n",
        "    })\n",
        "    print(f'\\n[{i+1}] {text[:70]}')\n",
        "    print(f'    NLTK  ({len(n_toks):>3} tokens, {nltk_ms:5.1f}ms): {n_toks[:8]}')\n",
        "    print(f'    spaCy ({len(s_toks):>3} tokens, {spacy_ms:5.1f}ms): {s_toks[:8]}')\n",
        "\n",
        "# ‚îÄ‚îÄ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "df_cmp = pd.DataFrame(comparison_rows)\n",
        "print('\\n' + '=' * 65)\n",
        "print('üìä ‡∏™‡∏£‡∏∏‡∏õ Tokenization Comparison (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 5 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)')\n",
        "print('=' * 65)\n",
        "print(f'  NLTK  ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {df_cmp[\"nltk_count\"].mean():>6.1f} tokens  | {df_cmp[\"nltk_time_ms\"].mean():>6.1f} ms')\n",
        "print(f'  spaCy ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {df_cmp[\"spacy_count\"].mean():>6.1f} tokens  | {df_cmp[\"spacy_time_ms\"].mean():>6.1f} ms')\n",
        "print()\n",
        "print('üìå ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:')\n",
        "print('  ‚Ä¢ NLTK  ‚Üí regex-based tokenizer, ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà')\n",
        "print('  ‚Ä¢ spaCy ‚Üí statistical model, ‡πÉ‡∏ä‡πâ lemmatization (running‚Üírun)')\n",
        "print('           ‡∏ó‡∏≥‡πÉ‡∏´‡πâ vocab ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å POS/NER ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß')\n",
        "print('  ‚Ä¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å: spaCy ‡πÅ‡∏¢‡∏Å punctuation ‡∏ä‡∏±‡∏î‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏•‡∏∞')\n",
        "print('    lemmatize ‡∏ó‡∏≥‡πÉ‡∏´‡πâ token count ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ NLTK ‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ')"
      ],
      "id": "pllVJiOBtSG1"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "kF42l8kYtSG2",
        "outputId": "d8c2e0bb-f9c8-4d84-909a-10d1e4ddb9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Apply tokenization ‡∏ö‡∏ô complete_dataset ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...\n",
            "   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: 6,095 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
            "\n",
            "üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ Token Count (‡∏ó‡∏±‡πâ‡∏á Dataset):\n",
            "   NLTK  ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 14.5  | min 1  | max 119\n",
            "   spaCy ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 14.2 | min 0 | max 115\n",
            "\n",
            "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 ‡πÅ‡∏ñ‡∏ß ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Sentence  count_nltk  count_spacy\n",
              "0  Three Top Dividend Stocks To Consider. As Febr...          39           36\n",
              "1  Dow Jones Futures Fall After S&P 500 Hits Resi...          27           24\n",
              "2  Here's Why I Wouldn't Touch Medical Properties...          16           15"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4abc8fa-eb3c-4727-8bca-8eb856135777\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>count_nltk</th>\n",
              "      <th>count_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Three Top Dividend Stocks To Consider. As Febr...</td>\n",
              "      <td>39</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dow Jones Futures Fall After S&amp;P 500 Hits Resi...</td>\n",
              "      <td>27</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Here's Why I Wouldn't Touch Medical Properties...</td>\n",
              "      <td>16</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4abc8fa-eb3c-4727-8bca-8eb856135777')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f4abc8fa-eb3c-4727-8bca-8eb856135777 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f4abc8fa-eb3c-4727-8bca-8eb856135777');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_complete[['Sentence', 'count_nltk', 'count_spacy']]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Three Top Dividend Stocks To Consider. As February begins, U.S. stock indexes have shown strength, with the Dow Jones Industrial Average adding 515 points and the S&P 500 nearing a record high. In this dynamic market environment, dividend stocks can offer stability and income potential, making them an appealing consideration for investors seeking to balance growth with consistent returns.\",\n          \"Dow Jones Futures Fall After S&P 500 Hits Resistance; Carvana Dives, Walmart Due. Futures fall after the stock market extended a bounce Wednesday, but the S&P 500 hit resistance. Carvana dived late. Walmart earnings loom.\",\n          \"Here's Why I Wouldn't Touch Medical Properties Trust With a 10 Foot Pole. Medical Properties Trust has a huge 6.6% yield, but the dividend history is a big problem for me.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count_nltk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 16,\n        \"max\": 39,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          39,\n          27,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 15,\n        \"max\": 36,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          36,\n          24,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  Apply Tokenization ‡∏ó‡∏±‡πâ‡∏á complete_dataset\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print('‚è≥ Apply tokenization ‡∏ö‡∏ô complete_dataset ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...')\n",
        "print(f'   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(df_complete):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£')\n",
        "\n",
        "df_complete['tokens_nltk']  = df_complete['Sentence'].apply(lambda x: tok_nltk(x[:3000]))\n",
        "df_complete['tokens_spacy'] = df_complete['Sentence'].apply(lambda x: tok_spacy(x[:3000]))\n",
        "df_complete['count_nltk']   = df_complete['tokens_nltk'].apply(len)\n",
        "df_complete['count_spacy']  = df_complete['tokens_spacy'].apply(len)\n",
        "\n",
        "print()\n",
        "print('üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ Token Count (‡∏ó‡∏±‡πâ‡∏á Dataset):')\n",
        "print(f'   NLTK  ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {df_complete[\"count_nltk\"].mean():.1f}  | min {df_complete[\"count_nltk\"].min()}  | max {df_complete[\"count_nltk\"].max()}')\n",
        "print(f'   spaCy ‚Äî ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {df_complete[\"count_spacy\"].mean():.1f} | min {df_complete[\"count_spacy\"].min()} | max {df_complete[\"count_spacy\"].max()}')\n",
        "\n",
        "print()\n",
        "print('--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 ‡πÅ‡∏ñ‡∏ß ---')\n",
        "df_complete[['Sentence', 'count_nltk', 'count_spacy']].head(3)"
      ],
      "id": "kF42l8kYtSG2"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmsMNrFCtSG2",
        "outputId": "73d55f94-dd43-4771-a92d-d4e2e890bddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checklist Phase 1 ‚Äî ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Phase 2:\n",
            "=================================================================\n",
            "  ‚úÖ clean_text_(Web_Scraping).csv ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß (839 rows)\n",
            "  ‚úÖ complete_dataset.csv ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß (6,095 rows)\n",
            "  ‚úÖ complete_dataset ‡∏°‡∏µ column \"Sentence\" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏Å‡πà‡∏≠‡∏ô tokenize)\n",
            "  ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà Sentence ‡∏ß‡πà‡∏≤‡∏á\n",
            "  ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ whitespace ‡∏ã‡πâ‡∏≥\n",
            "  ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏ã‡πâ‡∏≥\n",
            "  ‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå tokens_nltk\n",
            "  ‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå tokens_spacy\n",
            "  ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå raw_news.csv / processed_news.csv ‡πÉ‡∏ô output_data\n",
            "=================================================================\n",
            "\n",
            "‚úÖ ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç!\n",
            "   ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô Phase 2 ‡∏î‡πâ‡∏ß‡∏¢: pd.read_csv(\"output_data/complete_dataset.csv\")\n",
            "\n",
            "üìÅ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô output_data/:\n",
            "   ‚úÖ clean_text_(Web_Scraping).csv  (322.1 KB)\n",
            "   ‚úÖ complete_dataset.csv  (923.7 KB)\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#  CHECKLIST ‚Äî ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Phase 2 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á?\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print('üîç Checklist Phase 1 ‚Äî ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Phase 2:')\n",
        "print('=' * 65)\n",
        "\n",
        "web_ok      = os.path.exists(WEB_CSV)\n",
        "complete_ok = os.path.exists(COMPLETE_CSV)\n",
        "\n",
        "checks = {\n",
        "    f'clean_text_(Web_Scraping).csv ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß ({len(df_web):,} rows)':\n",
        "        web_ok,\n",
        "    f'complete_dataset.csv ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß ({len(df_complete):,} rows)':\n",
        "        complete_ok,\n",
        "    'complete_dataset ‡∏°‡∏µ column \"Sentence\" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏Å‡πà‡∏≠‡∏ô tokenize)':\n",
        "        'Sentence' in df_complete.columns,\n",
        "    '‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà Sentence ‡∏ß‡πà‡∏≤‡∏á':\n",
        "        (df_complete['Sentence'].str.strip() == '').sum() == 0,\n",
        "    '‡πÑ‡∏°‡πà‡∏°‡∏µ whitespace ‡∏ã‡πâ‡∏≥':\n",
        "        df_complete['Sentence'].str.contains(r'  ').sum() == 0,\n",
        "    '‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏ã‡πâ‡∏≥':\n",
        "        df_complete[['Sentence']].duplicated().sum() == 0,\n",
        "    '‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå tokens_nltk':\n",
        "        'tokens_nltk'  in df_complete.columns,\n",
        "    '‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå tokens_spacy':\n",
        "        'tokens_spacy' in df_complete.columns,\n",
        "    '‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå raw_news.csv / processed_news.csv ‡πÉ‡∏ô output_data':\n",
        "        not any(os.path.exists(f'{OUT_DIR}/{f}')\n",
        "                for f in ['raw_news.csv', 'processed_news.csv']),\n",
        "}\n",
        "\n",
        "all_pass = True\n",
        "for check, passed in checks.items():\n",
        "    icon = '‚úÖ' if passed else '‚ùå'\n",
        "    print(f'  {icon} {check}')\n",
        "    if not passed:\n",
        "        all_pass = False\n",
        "\n",
        "print('=' * 65)\n",
        "if all_pass:\n",
        "    print('\\n‚úÖ ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç!')\n",
        "    print(f'   ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô Phase 2 ‡∏î‡πâ‡∏ß‡∏¢: pd.read_csv(\"{COMPLETE_CSV}\")')\n",
        "else:\n",
        "    print('\\n‚ö†Ô∏è  ‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏Å‡πà‡∏≠‡∏ô')\n",
        "\n",
        "print()\n",
        "print(f'üìÅ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô {OUT_DIR}/:')\n",
        "for f in sorted(os.listdir(OUT_DIR)):\n",
        "    if f.endswith('.csv') and not f.startswith('_'):\n",
        "        size = os.path.getsize(f'{OUT_DIR}/{f}') / 1024\n",
        "        print(f'   ‚úÖ {f}  ({size:.1f} KB)')"
      ],
      "id": "KmsMNrFCtSG2"
    }
  ]
}
